{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "Notebook para testar dois modelos convolucionais com o dataset dos blobs.\n",
        "Créditos para o vídeo: [link text](https://www.youtube.com/watch?v=XZ7FYAMCc4M)\n"
      ],
      "metadata": {
        "id": "jm4QYqa98jtW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy6RyHOl-14s"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the dataset, who is a .zip file"
      ],
      "metadata": {
        "id": "PyQa2TgmCdpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nii.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWtbkurPCebD",
        "outputId": "946af8c4-9d5b-41be-add4-437bb3507074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nii.zip\n",
            "  inflating: nii/classes.txt         \n",
            "  inflating: nii/imagem_1.jpg        \n",
            "  inflating: nii/imagem_1.xml        \n",
            "  inflating: nii/imagem_10.jpg       \n",
            "  inflating: nii/imagem_10.xml       \n",
            "  inflating: nii/imagem_11.jpg       \n",
            "  inflating: nii/imagem_11.xml       \n",
            "  inflating: nii/imagem_12.jpg       \n",
            "  inflating: nii/imagem_12.xml       \n",
            "  inflating: nii/imagem_13.jpg       \n",
            "  inflating: nii/imagem_13.xml       \n",
            "  inflating: nii/imagem_14.jpg       \n",
            "  inflating: nii/imagem_14.xml       \n",
            "  inflating: nii/imagem_15.jpg       \n",
            "  inflating: nii/imagem_15.xml       \n",
            "  inflating: nii/imagem_16.jpg       \n",
            "  inflating: nii/imagem_16.xml       \n",
            "  inflating: nii/imagem_17.jpg       \n",
            "  inflating: nii/imagem_17.xml       \n",
            "  inflating: nii/imagem_18.jpg       \n",
            "  inflating: nii/imagem_18.xml       \n",
            "  inflating: nii/imagem_19.jpg       \n",
            "  inflating: nii/imagem_19.xml       \n",
            "  inflating: nii/imagem_2.jpg        \n",
            "  inflating: nii/imagem_2.xml        \n",
            "  inflating: nii/imagem_20.jpg       \n",
            "  inflating: nii/imagem_20.xml       \n",
            "  inflating: nii/imagem_21.jpg       \n",
            "  inflating: nii/imagem_21.xml       \n",
            "  inflating: nii/imagem_22.jpg       \n",
            "  inflating: nii/imagem_22.xml       \n",
            "  inflating: nii/imagem_23.jpg       \n",
            "  inflating: nii/imagem_23.xml       \n",
            "  inflating: nii/imagem_24.jpg       \n",
            "  inflating: nii/imagem_24.xml       \n",
            "  inflating: nii/imagem_25.jpg       \n",
            "  inflating: nii/imagem_25.xml       \n",
            "  inflating: nii/imagem_26.jpg       \n",
            "  inflating: nii/imagem_26.xml       \n",
            "  inflating: nii/imagem_27.jpg       \n",
            "  inflating: nii/imagem_27.xml       \n",
            "  inflating: nii/imagem_28.jpg       \n",
            "  inflating: nii/imagem_28.xml       \n",
            "  inflating: nii/imagem_29.jpg       \n",
            "  inflating: nii/imagem_29.xml       \n",
            "  inflating: nii/imagem_3.jpg        \n",
            "  inflating: nii/imagem_3.xml        \n",
            "  inflating: nii/imagem_30.jpg       \n",
            "  inflating: nii/imagem_30.xml       \n",
            "  inflating: nii/imagem_31.jpg       \n",
            "  inflating: nii/imagem_31.xml       \n",
            "  inflating: nii/imagem_32.jpg       \n",
            "  inflating: nii/imagem_32.xml       \n",
            "  inflating: nii/imagem_33.jpg       \n",
            "  inflating: nii/imagem_33.xml       \n",
            "  inflating: nii/imagem_34.jpg       \n",
            "  inflating: nii/imagem_34.xml       \n",
            "  inflating: nii/imagem_35.jpg       \n",
            "  inflating: nii/imagem_35.xml       \n",
            "  inflating: nii/imagem_36.jpg       \n",
            "  inflating: nii/imagem_36.xml       \n",
            "  inflating: nii/imagem_37.jpg       \n",
            "  inflating: nii/imagem_37.xml       \n",
            "  inflating: nii/imagem_38.jpg       \n",
            "  inflating: nii/imagem_38.xml       \n",
            "  inflating: nii/imagem_39.jpg       \n",
            "  inflating: nii/imagem_39.xml       \n",
            "  inflating: nii/imagem_4.jpg        \n",
            "  inflating: nii/imagem_4.xml        \n",
            "  inflating: nii/imagem_40.jpg       \n",
            "  inflating: nii/imagem_40.xml       \n",
            "  inflating: nii/imagem_41.jpg       \n",
            "  inflating: nii/imagem_41.xml       \n",
            "  inflating: nii/imagem_42.jpg       \n",
            "  inflating: nii/imagem_42.xml       \n",
            "  inflating: nii/imagem_43.jpg       \n",
            "  inflating: nii/imagem_43.xml       \n",
            "  inflating: nii/imagem_44.jpg       \n",
            "  inflating: nii/imagem_44.xml       \n",
            "  inflating: nii/imagem_45.jpg       \n",
            "  inflating: nii/imagem_45.xml       \n",
            "  inflating: nii/imagem_46.jpg       \n",
            "  inflating: nii/imagem_46.xml       \n",
            "  inflating: nii/imagem_47.jpg       \n",
            "  inflating: nii/imagem_47.xml       \n",
            "  inflating: nii/imagem_48.jpg       \n",
            "  inflating: nii/imagem_48.xml       \n",
            "  inflating: nii/imagem_49.jpg       \n",
            "  inflating: nii/imagem_49.xml       \n",
            "  inflating: nii/imagem_5.jpg        \n",
            "  inflating: nii/imagem_5.xml        \n",
            "  inflating: nii/imagem_50.jpg       \n",
            "  inflating: nii/imagem_50.xml       \n",
            "  inflating: nii/imagem_51.jpg       \n",
            "  inflating: nii/imagem_51.xml       \n",
            "  inflating: nii/imagem_52.jpg       \n",
            "  inflating: nii/imagem_52.xml       \n",
            "  inflating: nii/imagem_53.jpg       \n",
            "  inflating: nii/imagem_53.xml       \n",
            "  inflating: nii/imagem_6.jpg        \n",
            "  inflating: nii/imagem_6.xml        \n",
            "  inflating: nii/imagem_7.jpg        \n",
            "  inflating: nii/imagem_7.xml        \n",
            "  inflating: nii/imagem_8.jpg        \n",
            "  inflating: nii/imagem_8.xml        \n",
            "  inflating: nii/imagem_9.jpg        \n",
            "  inflating: nii/imagem_9.xml        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv nii dataset"
      ],
      "metadata": {
        "id": "87xr7GFytoj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset shuffe\n",
        "\n",
        "O dataset está organizado com imagens do tipo .jpg e seus repectivos .txt com informações de classes de cada blob apresentado. O processo que será realizado é de separar uma porcentagem em treinamento e outra para validação."
      ],
      "metadata": {
        "id": "uMuODSW6Da3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_dataset(dataset_dir, train_dir, val_dir, split_ratio=0.8, seed=None):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    # Verifica se os diretórios de treinamento e validação existem e cria-os se não existirem\n",
        "    for directory in [train_dir, val_dir]:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "    # Lista todos os arquivos na pasta do dataset\n",
        "    files = os.listdir(dataset_dir)\n",
        "\n",
        "    # Filtra apenas os arquivos de imagem e texto\n",
        "    image_files = [file for file in files if file.endswith('.jpg')]\n",
        "    text_files = [file for file in files if file.endswith('.xml')]\n",
        "\n",
        "    # Embaralha os arquivos\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    # Calcula o número de imagens para o conjunto de treinamento\n",
        "    num_train = int(len(image_files) * split_ratio)\n",
        "\n",
        "    # Move os arquivos para os diretórios de treinamento e validação\n",
        "    for image_file in image_files[:num_train]:\n",
        "        xml_file = image_file.replace('.jpg', '.xml')\n",
        "        shutil.move(os.path.join(dataset_dir, image_file), os.path.join(train_dir, image_file))\n",
        "        shutil.move(os.path.join(dataset_dir, xml_file), os.path.join(train_dir, xml_file))\n",
        "\n",
        "    for image_file in image_files[num_train:]:\n",
        "        xml_file = image_file.replace('.jpg', '.xml')\n",
        "        shutil.move(os.path.join(dataset_dir, image_file), os.path.join(val_dir, image_file))\n",
        "        shutil.move(os.path.join(dataset_dir, xml_file), os.path.join(val_dir, xml_file))\n",
        "\n",
        "# Exemplo de uso\n",
        "dataset_dir = './dataset'\n",
        "train_dir = './train'\n",
        "val_dir = './validation'\n",
        "split_ratio = 0.8  # Proporção de imagens para treinamento (80%) e validação (20%)\n",
        "seed = 42  # Semente aleatória para reprodutibilidade\n",
        "\n",
        "split_dataset(dataset_dir, train_dir, val_dir, split_ratio, seed)"
      ],
      "metadata": {
        "id": "zyO8o4zzDdrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the tensorflow models repository from GitHub\n",
        "!pip uninstall Cython -y # Temporary fix for \"No module named 'object_detection'\" error\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skzrzaykD9kW",
        "outputId": "63cacfb2-d5e9-42de-c1b6-dc35609aef92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: Cython 3.0.8\n",
            "Uninstalling Cython-3.0.8:\n",
            "  Successfully uninstalled Cython-3.0.8\n",
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 4074, done.\u001b[K\n",
            "remote: Counting objects: 100% (4074/4074), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3053/3053), done.\u001b[K\n",
            "remote: Total 4074 (delta 1186), reused 2896 (delta 961), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4074/4074), 44.59 MiB | 35.40 MiB/s, done.\n",
            "Resolving deltas: 100% (1186/1186), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=."
      ],
      "metadata": {
        "id": "F7QtiLSsStwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify setup.py file to install the tf-models-official repository targeted at TF v2.8.0\n",
        "import re\n",
        "with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open('/content/models/research/setup.py', 'w') as f:\n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('tf-models-official>=2.5.1',\n",
        "               'tf-models-official==2.8.0', s)\n",
        "    f.write(s)"
      ],
      "metadata": {
        "id": "_yo7dTl2Syss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Object Detection API (NOTE: This block takes about 10 minutes to finish executing)\n",
        "\n",
        "# Need to do a temporary fix with PyYAML because Colab isn't able to install PyYAML v5.4.1\n",
        "!pip install pyyaml==5.3\n",
        "!pip install /content/models/research/\n",
        "\n",
        "# Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)\n",
        "!pip install tensorflow==2.8.0\n",
        "\n",
        "# Install CUDA version 11.0 (to maintain compatibility with TF v2.8.0)\n",
        "!pip install tensorflow_io==0.23.1\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
        "!mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\n",
        "!apt-get update && sudo apt-get install cuda-toolkit-11-0\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v6EEOl0iTITz",
        "outputId": "21cd4110-e1a3-474b-b14d-dd60dc67bd50"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyyaml==5.3\n",
            "  Downloading PyYAML-5.3.tar.gz (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.2/268.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3-cp310-cp310-linux_x86_64.whl size=44244 sha256=dba40e8c537c0fb507818b489ec734137313a8dc8513a78fea9e785b993a2adb\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/72/68/a263cfc14175636cf26bada99f13b735be1b60a11318e08bfc\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2023.8.1 requires pyyaml>=5.3.1, but you have pyyaml 5.3 which is incompatible.\n",
            "distributed 2023.8.1 requires pyyaml>=5.3.1, but you have pyyaml 5.3 which is incompatible.\n",
            "flax 0.8.1 requires PyYAML>=5.4.1, but you have pyyaml 5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyyaml-5.3\n",
            "Processing ./models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting avro-python3 (from object-detection==0.1)\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting apache-beam (from object-detection==0.1)\n",
            "  Downloading apache_beam-2.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (9.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (4.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.7.1)\n",
            "Collecting Cython (from object-detection==0.1)\n",
            "  Downloading Cython-3.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (21.6.0)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.0.7)\n",
            "Collecting lvis (from object-detection==0.1)\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.5.3)\n",
            "Collecting tf-models-official==2.8.0 (from object-detection==0.1)\n",
            "  Downloading tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow_io (from object-detection==0.1)\n",
            "  Downloading tensorflow_io-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.15.0)\n",
            "Collecting pyparsing==2.4.7 (from object-detection==0.1)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu<=2.2.0 (from object-detection==0.1)\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (2.84.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (1.5.16)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (1.25.2)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (4.9.0.80)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: pyyaml<6.0,>=5.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (5.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (0.1.99)\n",
            "Collecting seqeval (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorflow-addons (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (4.9.4)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object-detection==0.1) (0.16.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorflow_text-2.8.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl (498.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2023.4)\n",
            "Collecting portalocker (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.9.0)\n",
            "Collecting colorama (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim->object-detection==0.1) (1.4.0)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam->object-detection==0.1)\n",
            "  Downloading orjson-3.9.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam->object-detection==0.1)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam->object-detection==0.1)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.60.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.22.0)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam->object-detection==0.1)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.19.2)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.0.2)\n",
            "Collecting numpy>=1.15.4 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting objsize<0.8.0,>=0.6.1 (from apache-beam->object-detection==0.1)\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (23.2)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.2/677.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.9.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<15.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.6)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (1.4.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (4.8.0.76)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (4.49.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.36.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io->object-detection==0.1) (0.36.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (4.1.1)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam->object-detection==0.1) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam->object-detection==0.1)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (0.18.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (2024.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (6.1.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (23.5.26)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.9.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.3.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow~=2.8.0 (from tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorflow-2.8.3-cp310-cp310-manylinux2010_x86_64.whl (498.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.5/498.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.8.2-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.8.1-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.9,>=2.8 (from tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras (from object-detection==0.1)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.6.0->tf-models-official==2.8.0->object-detection==0.1) (2.15.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.8.0->object-detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object-detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object-detection==0.1) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.8.0->object-detection==0.1) (1.2.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (1.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (0.42.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.8.0->object-detection==0.1) (3.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (1.62.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.8.0->object-detection==0.1) (5.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.8.0->object-detection==0.1) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.8.0->object-detection==0.1) (3.3.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.5.2)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.8.0->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (2.1.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object-detection==0.1) (3.2.2)\n",
            "Building wheels for collected packages: object-detection, avro-python3, crcmod, dill, hdfs, seqeval, pyjsparser, docopt\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1697352 sha256=24efff8d19db22f7153afc41abff645c1fe9a4fce692eaec4b092499f692619f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sqa_m7pi/wheels/53/dd/70/2de274d6c443c69d367bd6a5606f95e5a6df61aacf1435ec0d\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43992 sha256=554533bcdbd490d432334cdf237dbb42a466b1aca8818e1b611ae11d1a4e8bf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31405 sha256=4ddc107011350a468b8c7d71b85ec038cc8c176fa2435ffd437729cb6d59c372\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78540 sha256=d34e990aae5e56e0d568f2fe8e61372ced68b8ebeb0872bafbb37162ef7be78a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=f0391f22a34779dccc580bfbb0eced61027f06609a99f0fd408be4cfcfe79e0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=04933e98831fa4c810d38f0929cbf5d5a6c2cdd4a5f141e25cff521732dbbc29\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=1074edb3bd790ae4efefca4e58aefaa4ada3c7fb6fc293a304553141b49af49a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=4239a06b314b1b8dcd970297eb97bf693e5de0d2ed2f73bfceb6c44a0a538dd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built object-detection avro-python3 crcmod dill hdfs seqeval pyjsparser docopt\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, pyjsparser, keras, docopt, crcmod, zstandard, typeguard, tensorflow_io, tensorboard-data-server, pyparsing, portalocker, orjson, objsize, numpy, js2py, fasteners, fastavro, dnspython, dill, Cython, colorama, avro-python3, tensorflow-model-optimization, tensorflow-addons, sacrebleu, pymongo, keras-preprocessing, hdfs, google-auth-oauthlib, tensorboard, seqeval, lvis, apache-beam, tensorflow, tensorflow-text, tf-models-official, object-detection\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.8.1 requires PyYAML>=5.4.1, but you have pyyaml 5.3 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Cython-3.0.8 apache-beam-2.54.0 avro-python3-1.10.2 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 fastavro-1.9.4 fasteners-0.19 google-auth-oauthlib-0.4.6 hdfs-2.7.3 js2py-0.74 keras-2.8.0 keras-preprocessing-1.1.2 lvis-0.5.3 numpy-1.24.4 object-detection-0.1 objsize-0.7.0 orjson-3.9.14 portalocker-2.8.2 pyjsparser-2.7.1 pymongo-4.6.2 pyparsing-2.4.7 sacrebleu-2.2.0 seqeval-1.2.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.1 tensorflow-addons-0.23.0 tensorflow-estimator-2.8.0 tensorflow-model-optimization-0.8.0 tensorflow-text-2.8.2 tensorflow_io-0.36.0 tf-models-official-2.8.0 typeguard-2.13.3 zstandard-0.22.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "numpy",
                  "pyparsing",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.5.26)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.24.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.60.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.1\n",
            "    Uninstalling tensorflow-2.8.1:\n",
            "      Successfully uninstalled tensorflow-2.8.1\n",
            "Successfully installed tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_io==0.23.1\n",
            "  Downloading tensorflow_io-0.23.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.23.1 (from tensorflow_io==0.23.1)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-io-gcs-filesystem, tensorflow_io\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.36.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.36.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.36.0\n",
            "  Attempting uninstall: tensorflow_io\n",
            "    Found existing installation: tensorflow-io 0.36.0\n",
            "    Uninstalling tensorflow-io-0.36.0:\n",
            "      Successfully uninstalled tensorflow-io-0.36.0\n",
            "Successfully installed tensorflow-io-gcs-filesystem-0.23.1 tensorflow_io-0.23.1\n",
            "--2024-02-22 12:14:50--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190 [application/octet-stream]\n",
            "Saving to: ‘cuda-ubuntu1804.pin’\n",
            "\n",
            "cuda-ubuntu1804.pin 100%[===================>]     190  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-22 12:14:50 (4.82 MB/s) - ‘cuda-ubuntu1804.pin’ saved [190/190]\n",
            "\n",
            "--2024-02-22 12:14:50--  http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb [following]\n",
            "--2024-02-22 12:14:50--  https://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2273753684 (2.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu180 100%[===================>]   2.12G   205MB/s    in 9.8s    \n",
            "\n",
            "2024-02-22 12:15:00 (221 MB/s) - ‘cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb’ saved [2273753684/2273753684]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1804-11-0-local.\n",
            "(Reading database ... 121749 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1804-11-0-local (11.0.2-450.51.05-1) ...\n",
            "Setting up cuda-repo-ubuntu1804-11-0-local (11.0.2-450.51.05-1) ...\n",
            "\n",
            "The public CUDA GPG key does not appear to be installed.\n",
            "To install the key, run this command:\n",
            "sudo apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\n",
            "\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-ubuntu1804-11-0-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu1804-11-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu1804-11-0-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu1804-11-0-local  Release [564 B]\n",
            "Get:3 file:/var/cuda-repo-ubuntu1804-11-0-local  Release.gpg [836 B]\n",
            "Get:3 file:/var/cuda-repo-ubuntu1804-11-0-local  Release.gpg [836 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:9 file:/var/cuda-repo-ubuntu1804-11-0-local  Packages [23.9 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,742 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 1,975 kB in 1s (1,663 kB/s)\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu1804-11-0-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-11-0 cuda-cudart-11-0\n",
            "  cuda-cudart-dev-11-0 cuda-cuobjdump-11-0 cuda-cupti-11-0 cuda-cupti-dev-11-0\n",
            "  cuda-documentation-11-0 cuda-driver-dev-11-0 cuda-gdb-11-0\n",
            "  cuda-libraries-11-0 cuda-libraries-dev-11-0 cuda-memcheck-11-0\n",
            "  cuda-nsight-11-0 cuda-nsight-compute-11-0 cuda-nsight-systems-11-0\n",
            "  cuda-nvcc-11-0 cuda-nvdisasm-11-0 cuda-nvml-dev-11-0 cuda-nvprof-11-0\n",
            "  cuda-nvprune-11-0 cuda-nvrtc-11-0 cuda-nvrtc-dev-11-0 cuda-nvtx-11-0\n",
            "  cuda-nvvp-11-0 cuda-samples-11-0 cuda-sanitizer-11-0 cuda-tools-11-0\n",
            "  cuda-visual-tools-11-0 default-jre default-jre-headless fonts-dejavu-core\n",
            "  fonts-dejavu-extra freeglut3 freeglut3-dev libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-11-0 libcublas-dev-11-0 libcufft-11-0\n",
            "  libcufft-dev-11-0 libcurand-11-0 libcurand-dev-11-0 libcusolver-11-0\n",
            "  libcusolver-dev-11-0 libcusparse-11-0 libcusparse-dev-11-0 libegl-dev\n",
            "  libfontenc1 libgl-dev libgl1-mesa-dev libgles-dev libgles1 libglu1-mesa\n",
            "  libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libice-dev\n",
            "  libnpp-11-0 libnpp-dev-11-0 libnvjpeg-11-0 libnvjpeg-dev-11-0 libopengl-dev\n",
            "  libsm-dev libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxfixes-dev libxi-dev libxkbcommon-x11-0 libxkbfile1 libxmu-dev\n",
            "  libxmu-headers libxt-dev libxtst6 libxxf86dga1 nsight-systems-2023.3.3\n",
            "  openjdk-11-jre x11-utils\n",
            "Suggested packages:\n",
            "  libice-doc libsm-doc libxt-doc mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-11-0 cuda-cudart-11-0\n",
            "  cuda-cudart-dev-11-0 cuda-cuobjdump-11-0 cuda-cupti-11-0 cuda-cupti-dev-11-0\n",
            "  cuda-documentation-11-0 cuda-driver-dev-11-0 cuda-gdb-11-0\n",
            "  cuda-libraries-11-0 cuda-libraries-dev-11-0 cuda-memcheck-11-0\n",
            "  cuda-nsight-11-0 cuda-nsight-compute-11-0 cuda-nsight-systems-11-0\n",
            "  cuda-nvcc-11-0 cuda-nvdisasm-11-0 cuda-nvml-dev-11-0 cuda-nvprof-11-0\n",
            "  cuda-nvprune-11-0 cuda-nvrtc-11-0 cuda-nvrtc-dev-11-0 cuda-nvtx-11-0\n",
            "  cuda-nvvp-11-0 cuda-samples-11-0 cuda-sanitizer-11-0 cuda-toolkit-11-0\n",
            "  cuda-tools-11-0 cuda-visual-tools-11-0 default-jre default-jre-headless\n",
            "  fonts-dejavu-core fonts-dejavu-extra freeglut3 freeglut3-dev\n",
            "  libatk-wrapper-java libatk-wrapper-java-jni libcublas-11-0\n",
            "  libcublas-dev-11-0 libcufft-11-0 libcufft-dev-11-0 libcurand-11-0\n",
            "  libcurand-dev-11-0 libcusolver-11-0 libcusolver-dev-11-0 libcusparse-11-0\n",
            "  libcusparse-dev-11-0 libegl-dev libfontenc1 libgl-dev libgl1-mesa-dev\n",
            "  libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n",
            "  libglvnd-dev libglx-dev libice-dev libnpp-11-0 libnpp-dev-11-0\n",
            "  libnvjpeg-11-0 libnvjpeg-dev-11-0 libopengl-dev libsm-dev libtinfo5\n",
            "  libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0 libxcb-util1\n",
            "  libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxfixes-dev libxi-dev\n",
            "  libxkbcommon-x11-0 libxkbfile1 libxmu-dev libxmu-headers libxt-dev libxtst6\n",
            "  libxxf86dga1 nsight-systems-2023.3.3 openjdk-11-jre x11-utils\n",
            "0 upgraded, 87 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 330 MB/1,911 MB of archives.\n",
            "After this operation, 4,023 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-cudart-11-0 11.0.194-1 [129 kB]\n",
            "Get:2 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-driver-dev-11-0 11.0.194-1 [25.0 kB]\n",
            "Get:3 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-cudart-dev-11-0 11.0.194-1 [1,662 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.3.3 2023.3.3.42-233333266658v0 [324 MB]\n",
            "Get:5 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvcc-11-0 11.0.194-1 [21.1 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:7 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-cupti-11-0 11.0.194-1 [10.5 MB]\n",
            "Get:8 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-cupti-dev-11-0 11.0.194-1 [2,276 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:11 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvdisasm-11-0 11.0.194-1 [27.3 MB]\n",
            "Get:12 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-cuobjdump-11-0 11.0.194-1 [103 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:16 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-gdb-11-0 11.0.194-1 [3,891 kB]\n",
            "Get:17 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-memcheck-11-0 11.0.194-1 [144 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:19 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvprof-11-0 11.0.194-1 [1,911 kB]\n",
            "Get:20 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvtx-11-0 11.0.167-1 [51.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [6,842 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:25 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-sanitizer-11-0 11.0.194-1 [7,220 kB]\n",
            "Get:26 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-command-line-tools-11-0 11.0.2-1 [2,474 B]\n",
            "Get:27 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvprune-11-0 11.0.167-1 [53.1 kB]\n",
            "Get:28 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-compiler-11-0 11.0.2-1 [2,416 B]\n",
            "Get:29 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvrtc-11-0 11.0.194-1 [6,521 kB]\n",
            "Get:30 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvrtc-dev-11-0 11.0.194-1 [22.1 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxmu-headers all 2:1.1.3-3 [54.1 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxmu-dev amd64 2:1.1.3-3 [54.6 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfixes-dev amd64 1:6.0.0-1 [12.2 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxi-dev amd64 2:1.8-1build1 [193 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.21+9-0ubuntu1~22.04 [214 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:62 file:/var/cuda-repo-ubuntu1804-11-0-local  libcusolver-11-0 10.5.0.218-1 [277 MB]\n",
            "Get:63 file:/var/cuda-repo-ubuntu1804-11-0-local  libcusolver-dev-11-0 10.5.0.218-1 [17.6 MB]\n",
            "Get:64 file:/var/cuda-repo-ubuntu1804-11-0-local  libcublas-11-0 11.1.0.229-1 [118 MB]\n",
            "Get:65 file:/var/cuda-repo-ubuntu1804-11-0-local  libcublas-dev-11-0 11.1.0.229-1 [120 MB]\n",
            "Get:66 file:/var/cuda-repo-ubuntu1804-11-0-local  libcufft-11-0 10.2.0.218-1 [94.1 MB]\n",
            "Get:67 file:/var/cuda-repo-ubuntu1804-11-0-local  libcufft-dev-11-0 10.2.0.218-1 [172 MB]\n",
            "Get:68 file:/var/cuda-repo-ubuntu1804-11-0-local  libcurand-11-0 10.2.1.218-1 [39.2 MB]\n",
            "Get:69 file:/var/cuda-repo-ubuntu1804-11-0-local  libcurand-dev-11-0 10.2.1.218-1 [39.2 MB]\n",
            "Get:70 file:/var/cuda-repo-ubuntu1804-11-0-local  libcusparse-11-0 11.1.0.218-1 [71.2 MB]\n",
            "Get:71 file:/var/cuda-repo-ubuntu1804-11-0-local  libcusparse-dev-11-0 11.1.0.218-1 [71.4 MB]\n",
            "Get:72 file:/var/cuda-repo-ubuntu1804-11-0-local  libnpp-11-0 11.1.0.218-1 [56.6 MB]\n",
            "Get:73 file:/var/cuda-repo-ubuntu1804-11-0-local  libnpp-dev-11-0 11.1.0.218-1 [57.4 MB]\n",
            "Get:74 file:/var/cuda-repo-ubuntu1804-11-0-local  libnvjpeg-11-0 11.1.0.218-1 [1,391 kB]\n",
            "Get:75 file:/var/cuda-repo-ubuntu1804-11-0-local  libnvjpeg-dev-11-0 11.1.0.218-1 [1,321 kB]\n",
            "Get:76 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-samples-11-0 11.0.194-1 [68.1 MB]\n",
            "Get:77 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-documentation-11-0 11.0.207-1 [59.6 MB]\n",
            "Get:78 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-libraries-11-0 11.0.2-1 [2,490 B]\n",
            "Get:79 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-libraries-dev-11-0 11.0.2-1 [2,514 B]\n",
            "Get:80 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nsight-11-0 11.0.194-1 [119 MB]\n",
            "Get:81 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nsight-compute-11-0 11.0.2-1 [3,718 B]\n",
            "Get:82 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nsight-systems-11-0 11.0.2-1 [3,280 B]\n",
            "Get:83 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvml-dev-11-0 11.0.167-1 [71.9 kB]\n",
            "Get:84 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-nvvp-11-0 11.0.194-1 [115 MB]\n",
            "Get:85 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-visual-tools-11-0 11.0.2-1 [2,942 B]\n",
            "Get:86 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-tools-11-0 11.0.2-1 [2,380 B]\n",
            "Get:87 file:/var/cuda-repo-ubuntu1804-11-0-local  cuda-toolkit-11-0 11.0.2-1 [2,728 B]\n",
            "Fetched 330 MB in 16s (20.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 87.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cuda-cudart-11-0.\n",
            "(Reading database ... 121836 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-cudart-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-11-0.\n",
            "Preparing to unpack .../01-cuda-driver-dev-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-11-0.\n",
            "Preparing to unpack .../02-cuda-cudart-dev-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-11-0.\n",
            "Preparing to unpack .../03-cuda-nvcc-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-cupti-11-0.\n",
            "Preparing to unpack .../04-cuda-cupti-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-11-0.\n",
            "Preparing to unpack .../05-cuda-cupti-dev-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-11-0.\n",
            "Preparing to unpack .../06-cuda-nvdisasm-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-11-0.\n",
            "Preparing to unpack .../07-cuda-cuobjdump-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-gdb-11-0.\n",
            "Preparing to unpack .../08-cuda-gdb-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-memcheck-11-0.\n",
            "Preparing to unpack .../09-cuda-memcheck-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-memcheck-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-11-0.\n",
            "Preparing to unpack .../10-cuda-nvprof-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-11-0.\n",
            "Preparing to unpack .../11-cuda-nvtx-11-0_11.0.167-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-11-0 (11.0.167-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-11-0.\n",
            "Preparing to unpack .../12-cuda-sanitizer-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-11-0.\n",
            "Preparing to unpack .../13-cuda-command-line-tools-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-11-0.\n",
            "Preparing to unpack .../14-cuda-nvprune-11-0_11.0.167-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-11-0 (11.0.167-1) ...\n",
            "Selecting previously unselected package cuda-compiler-11-0.\n",
            "Preparing to unpack .../15-cuda-compiler-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "Preparing to unpack .../16-freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "Preparing to unpack .../17-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../18-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../19-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../20-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../21-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../22-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../23-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../24-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../25-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../26-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../27-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libice-dev:amd64.\n",
            "Preparing to unpack .../28-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n",
            "Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Selecting previously unselected package libsm-dev:amd64.\n",
            "Preparing to unpack .../29-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n",
            "Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../30-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package freeglut3-dev:amd64.\n",
            "Preparing to unpack .../31-freeglut3-dev_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libxmu-headers.\n",
            "Preparing to unpack .../32-libxmu-headers_2%3a1.1.3-3_all.deb ...\n",
            "Unpacking libxmu-headers (2:1.1.3-3) ...\n",
            "Selecting previously unselected package libxmu-dev:amd64.\n",
            "Preparing to unpack .../33-libxmu-dev_2%3a1.1.3-3_amd64.deb ...\n",
            "Unpacking libxmu-dev:amd64 (2:1.1.3-3) ...\n",
            "Selecting previously unselected package libxfixes-dev:amd64.\n",
            "Preparing to unpack .../34-libxfixes-dev_1%3a6.0.0-1_amd64.deb ...\n",
            "Unpacking libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Selecting previously unselected package libxi-dev:amd64.\n",
            "Preparing to unpack .../35-libxi-dev_2%3a1.8-1build1_amd64.deb ...\n",
            "Unpacking libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-11-0.\n",
            "Preparing to unpack .../36-cuda-nvrtc-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-11-0.\n",
            "Preparing to unpack .../37-cuda-nvrtc-dev-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package libcusolver-11-0.\n",
            "Preparing to unpack .../38-libcusolver-11-0_10.5.0.218-1_amd64.deb ...\n",
            "Unpacking libcusolver-11-0 (10.5.0.218-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-11-0.\n",
            "Preparing to unpack .../39-libcusolver-dev-11-0_10.5.0.218-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-11-0 (10.5.0.218-1) ...\n",
            "Selecting previously unselected package libcublas-11-0.\n",
            "Preparing to unpack .../40-libcublas-11-0_11.1.0.229-1_amd64.deb ...\n",
            "Unpacking libcublas-11-0 (11.1.0.229-1) ...\n",
            "Selecting previously unselected package libcublas-dev-11-0.\n",
            "Preparing to unpack .../41-libcublas-dev-11-0_11.1.0.229-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-11-0 (11.1.0.229-1) ...\n",
            "Selecting previously unselected package libcufft-11-0.\n",
            "Preparing to unpack .../42-libcufft-11-0_10.2.0.218-1_amd64.deb ...\n",
            "Unpacking libcufft-11-0 (10.2.0.218-1) ...\n",
            "Selecting previously unselected package libcufft-dev-11-0.\n",
            "Preparing to unpack .../43-libcufft-dev-11-0_10.2.0.218-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-11-0 (10.2.0.218-1) ...\n",
            "Selecting previously unselected package libcurand-11-0.\n",
            "Preparing to unpack .../44-libcurand-11-0_10.2.1.218-1_amd64.deb ...\n",
            "Unpacking libcurand-11-0 (10.2.1.218-1) ...\n",
            "Selecting previously unselected package libcurand-dev-11-0.\n",
            "Preparing to unpack .../45-libcurand-dev-11-0_10.2.1.218-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-11-0 (10.2.1.218-1) ...\n",
            "Selecting previously unselected package libcusparse-11-0.\n",
            "Preparing to unpack .../46-libcusparse-11-0_11.1.0.218-1_amd64.deb ...\n",
            "Unpacking libcusparse-11-0 (11.1.0.218-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-11-0.\n",
            "Preparing to unpack .../47-libcusparse-dev-11-0_11.1.0.218-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-11-0 (11.1.0.218-1) ...\n",
            "Selecting previously unselected package libnpp-11-0.\n",
            "Preparing to unpack .../48-libnpp-11-0_11.1.0.218-1_amd64.deb ...\n",
            "Unpacking libnpp-11-0 (11.1.0.218-1) ...\n",
            "Selecting previously unselected package libnpp-dev-11-0.\n",
            "Preparing to unpack .../49-libnpp-dev-11-0_11.1.0.218-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-11-0 (11.1.0.218-1) ...\n",
            "Selecting previously unselected package libnvjpeg-11-0.\n",
            "Preparing to unpack .../50-libnvjpeg-11-0_11.1.0.218-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-11-0 (11.1.0.218-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-11-0.\n",
            "Preparing to unpack .../51-libnvjpeg-dev-11-0_11.1.0.218-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-11-0 (11.1.0.218-1) ...\n",
            "Selecting previously unselected package cuda-samples-11-0.\n",
            "Preparing to unpack .../52-cuda-samples-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-samples-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-documentation-11-0.\n",
            "Preparing to unpack .../53-cuda-documentation-11-0_11.0.207-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-11-0 (11.0.207-1) ...\n",
            "Selecting previously unselected package cuda-libraries-11-0.\n",
            "Preparing to unpack .../54-cuda-libraries-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-11-0.\n",
            "Preparing to unpack .../55-cuda-libraries-dev-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../56-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../57-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../58-openjdk-11-jre_11.0.21+9-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.21+9-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../59-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-11-0.\n",
            "Preparing to unpack .../60-cuda-nsight-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-11-0.\n",
            "Preparing to unpack .../61-cuda-nsight-compute-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "Preparing to unpack .../62-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../63-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../64-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../65-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../66-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../67-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../68-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../69-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../70-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../71-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package nsight-systems-2023.3.3.\n",
            "Preparing to unpack .../72-nsight-systems-2023.3.3_2023.3.3.42-233333266658v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2023.3.3 (2023.3.3.42-233333266658v0) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-11-0.\n",
            "Preparing to unpack .../73-cuda-nsight-systems-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-11-0.\n",
            "Preparing to unpack .../74-cuda-nvml-dev-11-0_11.0.167-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-11-0 (11.0.167-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-11-0.\n",
            "Preparing to unpack .../75-cuda-nvvp-11-0_11.0.194-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-11-0 (11.0.194-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-11-0.\n",
            "Preparing to unpack .../76-cuda-visual-tools-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package cuda-tools-11-0.\n",
            "Preparing to unpack .../77-cuda-tools-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-tools-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-0.\n",
            "Preparing to unpack .../78-cuda-toolkit-11-0_11.0.2-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-11-0 (11.0.2-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../79-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../80-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../81-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../82-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../83-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../84-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../85-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../86-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up libcurand-11-0 (10.2.1.218-1) ...\n",
            "Setting up libcublas-11-0 (11.1.0.229-1) ...\n",
            "Setting up libxmu-headers (2:1.1.3-3) ...\n",
            "Setting up cuda-nvtx-11-0 (11.0.167-1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libcusolver-11-0 (10.5.0.218-1) ...\n",
            "Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Setting up cuda-driver-dev-11-0 (11.0.194-1) ...\n",
            "Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Setting up cuda-nsight-compute-11-0 (11.0.2-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-memcheck-11-0 (11.0.194-1) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.21+9-0ubuntu1~22.04) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up cuda-nvprune-11-0 (11.0.167-1) ...\n",
            "Setting up libnvjpeg-11-0 (11.1.0.218-1) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up cuda-cudart-11-0 (11.0.194-1) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Setting up cuda-nvprof-11-0 (11.0.194-1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up cuda-nvml-dev-11-0 (11.0.167-1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libcusparse-11-0 (11.1.0.218-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up cuda-cuobjdump-11-0 (11.0.194-1) ...\n",
            "Setting up libcufft-11-0 (10.2.0.218-1) ...\n",
            "Setting up cuda-cudart-dev-11-0 (11.0.194-1) ...\n",
            "Setting up cuda-nvrtc-11-0 (11.0.194-1) ...\n",
            "Setting up cuda-sanitizer-11-0 (11.0.194-1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libcufft-dev-11-0 (10.2.0.218-1) ...\n",
            "Setting up libnpp-11-0 (11.1.0.218-1) ...\n",
            "Setting up libcusolver-dev-11-0 (10.5.0.218-1) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up cuda-nvdisasm-11-0 (11.0.194-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up libcublas-dev-11-0 (11.1.0.229-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libcusparse-dev-11-0 (11.1.0.218-1) ...\n",
            "Setting up nsight-systems-2023.3.3 (2023.3.3.42-233333266658v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.3.3/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.3.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up cuda-nsight-systems-11-0 (11.0.2-1) ...\n",
            "Setting up cuda-nvvp-11-0 (11.0.194-1) ...\n",
            "Setting up libcurand-dev-11-0 (10.2.1.218-1) ...\n",
            "Setting up libnpp-dev-11-0 (11.1.0.218-1) ...\n",
            "Setting up cuda-libraries-11-0 (11.0.2-1) ...\n",
            "Setting up cuda-gdb-11-0 (11.0.194-1) ...\n",
            "Setting up cuda-nvrtc-dev-11-0 (11.0.194-1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up cuda-nsight-11-0 (11.0.194-1) ...\n",
            "Setting up libnvjpeg-dev-11-0 (11.1.0.218-1) ...\n",
            "Setting up libxmu-dev:amd64 (2:1.1.3-3) ...\n",
            "Setting up cuda-nvcc-11-0 (11.0.194-1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up cuda-libraries-dev-11-0 (11.0.2-1) ...\n",
            "Setting up cuda-cupti-11-0 (11.0.194-1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up cuda-visual-tools-11-0 (11.0.2-1) ...\n",
            "Setting up cuda-compiler-11-0 (11.0.2-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up cuda-cupti-dev-11-0 (11.0.194-1) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up cuda-command-line-tools-11-0 (11.0.2-1) ...\n",
            "Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n",
            "Setting up cuda-tools-11-0 (11.0.2-1) ...\n",
            "Setting up cuda-samples-11-0 (11.0.194-1) ...\n",
            "Setting up cuda-documentation-11-0 (11.0.207-1) ...\n",
            "Setting up cuda-toolkit-11-0 (11.0.2-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Model Bulider Test file, just to verify everything's working properly\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikpx4U3aVIis",
        "outputId": "438a6722-ee0f-4683-cbb7-541451e1f377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests under Python 3.10.12: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "2024-02-22 12:33:17.029637: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "W0222 12:33:17.287649 133585517813760 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 2.97s\n",
            "I0222 12:33:17.680091 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 2.97s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.52s\n",
            "I0222 12:33:18.204847 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.52s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.24s\n",
            "I0222 12:33:18.449441 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.24s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.22s\n",
            "I0222 12:33:18.672313 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.22s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.78s\n",
            "I0222 12:33:20.452447 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.78s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0222 12:33:20.453675 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
            "I0222 12:33:20.476143 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "I0222 12:33:20.490620 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "I0222 12:33:20.504534 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.09s\n",
            "I0222 12:33:20.597350 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.09s\n",
            "I0222 12:33:20.683359 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.09s\n",
            "I0222 12:33:20.776765 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.09s\n",
            "I0222 12:33:20.868211 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.09s\n",
            "I0222 12:33:20.958258 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n",
            "I0222 12:33:20.986492 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0222 12:33:21.157619 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0222 12:33:21.157766 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 64\n",
            "I0222 12:33:21.157865 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 3\n",
            "I0222 12:33:21.160417 133585517813760 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0222 12:33:21.176794 133585517813760 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0222 12:33:21.176919 133585517813760 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0222 12:33:21.244022 133585517813760 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0222 12:33:21.244187 133585517813760 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0222 12:33:21.394706 133585517813760 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0222 12:33:21.394859 133585517813760 efficientnet_model.py:144] round_filter input=40 output=40\n",
            "I0222 12:33:21.539403 133585517813760 efficientnet_model.py:144] round_filter input=40 output=40\n",
            "I0222 12:33:21.539552 133585517813760 efficientnet_model.py:144] round_filter input=80 output=80\n",
            "I0222 12:33:21.760496 133585517813760 efficientnet_model.py:144] round_filter input=80 output=80\n",
            "I0222 12:33:21.760656 133585517813760 efficientnet_model.py:144] round_filter input=112 output=112\n",
            "I0222 12:33:21.981129 133585517813760 efficientnet_model.py:144] round_filter input=112 output=112\n",
            "I0222 12:33:21.981325 133585517813760 efficientnet_model.py:144] round_filter input=192 output=192\n",
            "I0222 12:33:22.282710 133585517813760 efficientnet_model.py:144] round_filter input=192 output=192\n",
            "I0222 12:33:22.282889 133585517813760 efficientnet_model.py:144] round_filter input=320 output=320\n",
            "I0222 12:33:22.353612 133585517813760 efficientnet_model.py:144] round_filter input=1280 output=1280\n",
            "I0222 12:33:22.385134 133585517813760 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0222 12:33:22.434887 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I0222 12:33:22.435015 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 88\n",
            "I0222 12:33:22.435089 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 4\n",
            "I0222 12:33:22.436770 133585517813760 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0222 12:33:22.451057 133585517813760 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0222 12:33:22.451159 133585517813760 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0222 12:33:22.568350 133585517813760 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0222 12:33:22.568513 133585517813760 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0222 12:33:22.785973 133585517813760 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0222 12:33:22.786137 133585517813760 efficientnet_model.py:144] round_filter input=40 output=40\n",
            "I0222 12:33:23.210518 133585517813760 efficientnet_model.py:144] round_filter input=40 output=40\n",
            "I0222 12:33:23.210682 133585517813760 efficientnet_model.py:144] round_filter input=80 output=80\n",
            "I0222 12:33:23.522926 133585517813760 efficientnet_model.py:144] round_filter input=80 output=80\n",
            "I0222 12:33:23.523098 133585517813760 efficientnet_model.py:144] round_filter input=112 output=112\n",
            "I0222 12:33:23.815223 133585517813760 efficientnet_model.py:144] round_filter input=112 output=112\n",
            "I0222 12:33:23.815418 133585517813760 efficientnet_model.py:144] round_filter input=192 output=192\n",
            "I0222 12:33:24.180348 133585517813760 efficientnet_model.py:144] round_filter input=192 output=192\n",
            "I0222 12:33:24.180517 133585517813760 efficientnet_model.py:144] round_filter input=320 output=320\n",
            "I0222 12:33:24.319859 133585517813760 efficientnet_model.py:144] round_filter input=1280 output=1280\n",
            "I0222 12:33:24.345700 133585517813760 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0222 12:33:24.408048 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I0222 12:33:24.408217 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 112\n",
            "I0222 12:33:24.408308 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 5\n",
            "I0222 12:33:24.409899 133585517813760 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0222 12:33:24.424638 133585517813760 efficientnet_model.py:144] round_filter input=32 output=32\n",
            "I0222 12:33:24.424753 133585517813760 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0222 12:33:24.541496 133585517813760 efficientnet_model.py:144] round_filter input=16 output=16\n",
            "I0222 12:33:24.541633 133585517813760 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0222 12:33:24.749596 133585517813760 efficientnet_model.py:144] round_filter input=24 output=24\n",
            "I0222 12:33:24.749747 133585517813760 efficientnet_model.py:144] round_filter input=40 output=48\n",
            "I0222 12:33:24.965366 133585517813760 efficientnet_model.py:144] round_filter input=40 output=48\n",
            "I0222 12:33:24.965532 133585517813760 efficientnet_model.py:144] round_filter input=80 output=88\n",
            "I0222 12:33:25.260645 133585517813760 efficientnet_model.py:144] round_filter input=80 output=88\n",
            "I0222 12:33:25.260803 133585517813760 efficientnet_model.py:144] round_filter input=112 output=120\n",
            "I0222 12:33:25.556352 133585517813760 efficientnet_model.py:144] round_filter input=112 output=120\n",
            "I0222 12:33:25.556512 133585517813760 efficientnet_model.py:144] round_filter input=192 output=208\n",
            "I0222 12:33:25.920876 133585517813760 efficientnet_model.py:144] round_filter input=192 output=208\n",
            "I0222 12:33:25.921043 133585517813760 efficientnet_model.py:144] round_filter input=320 output=352\n",
            "I0222 12:33:26.073175 133585517813760 efficientnet_model.py:144] round_filter input=1280 output=1408\n",
            "I0222 12:33:26.112338 133585517813760 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0222 12:33:26.216248 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I0222 12:33:26.216433 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 160\n",
            "I0222 12:33:26.216502 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 6\n",
            "I0222 12:33:26.218686 133585517813760 efficientnet_model.py:144] round_filter input=32 output=40\n",
            "I0222 12:33:26.241369 133585517813760 efficientnet_model.py:144] round_filter input=32 output=40\n",
            "I0222 12:33:26.241523 133585517813760 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0222 12:33:26.407359 133585517813760 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0222 12:33:26.407533 133585517813760 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0222 12:33:26.702133 133585517813760 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0222 12:33:26.702328 133585517813760 efficientnet_model.py:144] round_filter input=40 output=48\n",
            "I0222 12:33:26.987982 133585517813760 efficientnet_model.py:144] round_filter input=40 output=48\n",
            "I0222 12:33:26.988173 133585517813760 efficientnet_model.py:144] round_filter input=80 output=96\n",
            "I0222 12:33:27.502287 133585517813760 efficientnet_model.py:144] round_filter input=80 output=96\n",
            "I0222 12:33:27.502474 133585517813760 efficientnet_model.py:144] round_filter input=112 output=136\n",
            "I0222 12:33:28.034421 133585517813760 efficientnet_model.py:144] round_filter input=112 output=136\n",
            "I0222 12:33:28.034628 133585517813760 efficientnet_model.py:144] round_filter input=192 output=232\n",
            "I0222 12:33:28.502152 133585517813760 efficientnet_model.py:144] round_filter input=192 output=232\n",
            "I0222 12:33:28.502333 133585517813760 efficientnet_model.py:144] round_filter input=320 output=384\n",
            "I0222 12:33:28.902870 133585517813760 efficientnet_model.py:144] round_filter input=1280 output=1536\n",
            "I0222 12:33:28.930349 133585517813760 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0222 12:33:28.992938 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0222 12:33:28.993074 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 224\n",
            "I0222 12:33:28.993148 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
            "I0222 12:33:28.994567 133585517813760 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0222 12:33:29.009205 133585517813760 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0222 12:33:29.009358 133585517813760 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0222 12:33:29.129122 133585517813760 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0222 12:33:29.129302 133585517813760 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0222 12:33:29.438528 133585517813760 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0222 12:33:29.438701 133585517813760 efficientnet_model.py:144] round_filter input=40 output=56\n",
            "I0222 12:33:29.732183 133585517813760 efficientnet_model.py:144] round_filter input=40 output=56\n",
            "I0222 12:33:29.732372 133585517813760 efficientnet_model.py:144] round_filter input=80 output=112\n",
            "I0222 12:33:30.167063 133585517813760 efficientnet_model.py:144] round_filter input=80 output=112\n",
            "I0222 12:33:30.167228 133585517813760 efficientnet_model.py:144] round_filter input=112 output=160\n",
            "I0222 12:33:30.613586 133585517813760 efficientnet_model.py:144] round_filter input=112 output=160\n",
            "I0222 12:33:30.613746 133585517813760 efficientnet_model.py:144] round_filter input=192 output=272\n",
            "I0222 12:33:31.204977 133585517813760 efficientnet_model.py:144] round_filter input=192 output=272\n",
            "I0222 12:33:31.205142 133585517813760 efficientnet_model.py:144] round_filter input=320 output=448\n",
            "I0222 12:33:31.354661 133585517813760 efficientnet_model.py:144] round_filter input=1280 output=1792\n",
            "I0222 12:33:31.384423 133585517813760 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0222 12:33:31.460969 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I0222 12:33:31.461113 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 288\n",
            "I0222 12:33:31.461186 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
            "I0222 12:33:31.462628 133585517813760 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0222 12:33:31.480449 133585517813760 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0222 12:33:31.480560 133585517813760 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0222 12:33:31.662030 133585517813760 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0222 12:33:31.662182 133585517813760 efficientnet_model.py:144] round_filter input=24 output=40\n",
            "I0222 12:33:32.028205 133585517813760 efficientnet_model.py:144] round_filter input=24 output=40\n",
            "I0222 12:33:32.028374 133585517813760 efficientnet_model.py:144] round_filter input=40 output=64\n",
            "I0222 12:33:32.394594 133585517813760 efficientnet_model.py:144] round_filter input=40 output=64\n",
            "I0222 12:33:32.394786 133585517813760 efficientnet_model.py:144] round_filter input=80 output=128\n",
            "I0222 12:33:32.897583 133585517813760 efficientnet_model.py:144] round_filter input=80 output=128\n",
            "I0222 12:33:32.897766 133585517813760 efficientnet_model.py:144] round_filter input=112 output=176\n",
            "I0222 12:33:33.406690 133585517813760 efficientnet_model.py:144] round_filter input=112 output=176\n",
            "I0222 12:33:33.406895 133585517813760 efficientnet_model.py:144] round_filter input=192 output=304\n",
            "I0222 12:33:34.057343 133585517813760 efficientnet_model.py:144] round_filter input=192 output=304\n",
            "I0222 12:33:34.057510 133585517813760 efficientnet_model.py:144] round_filter input=320 output=512\n",
            "I0222 12:33:34.586758 133585517813760 efficientnet_model.py:144] round_filter input=1280 output=2048\n",
            "I0222 12:33:34.614467 133585517813760 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0222 12:33:34.702317 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0222 12:33:34.702464 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
            "I0222 12:33:34.702539 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
            "I0222 12:33:34.704011 133585517813760 efficientnet_model.py:144] round_filter input=32 output=56\n",
            "I0222 12:33:34.720142 133585517813760 efficientnet_model.py:144] round_filter input=32 output=56\n",
            "I0222 12:33:34.720270 133585517813760 efficientnet_model.py:144] round_filter input=16 output=32\n",
            "I0222 12:33:34.900306 133585517813760 efficientnet_model.py:144] round_filter input=16 output=32\n",
            "I0222 12:33:34.900451 133585517813760 efficientnet_model.py:144] round_filter input=24 output=40\n",
            "I0222 12:33:35.336743 133585517813760 efficientnet_model.py:144] round_filter input=24 output=40\n",
            "I0222 12:33:35.336917 133585517813760 efficientnet_model.py:144] round_filter input=40 output=72\n",
            "I0222 12:33:35.797256 133585517813760 efficientnet_model.py:144] round_filter input=40 output=72\n",
            "I0222 12:33:35.797424 133585517813760 efficientnet_model.py:144] round_filter input=80 output=144\n",
            "I0222 12:33:36.384499 133585517813760 efficientnet_model.py:144] round_filter input=80 output=144\n",
            "I0222 12:33:36.384656 133585517813760 efficientnet_model.py:144] round_filter input=112 output=200\n",
            "I0222 12:33:36.982010 133585517813760 efficientnet_model.py:144] round_filter input=112 output=200\n",
            "I0222 12:33:36.982175 133585517813760 efficientnet_model.py:144] round_filter input=192 output=344\n",
            "I0222 12:33:37.781889 133585517813760 efficientnet_model.py:144] round_filter input=192 output=344\n",
            "I0222 12:33:37.782045 133585517813760 efficientnet_model.py:144] round_filter input=320 output=576\n",
            "I0222 12:33:37.990040 133585517813760 efficientnet_model.py:144] round_filter input=1280 output=2304\n",
            "I0222 12:33:38.019297 133585517813760 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0222 12:33:38.132898 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I0222 12:33:38.133097 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
            "I0222 12:33:38.133197 133585517813760 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
            "I0222 12:33:38.135481 133585517813760 efficientnet_model.py:144] round_filter input=32 output=64\n",
            "I0222 12:33:38.157304 133585517813760 efficientnet_model.py:144] round_filter input=32 output=64\n",
            "I0222 12:33:38.157439 133585517813760 efficientnet_model.py:144] round_filter input=16 output=32\n",
            "I0222 12:33:38.481323 133585517813760 efficientnet_model.py:144] round_filter input=16 output=32\n",
            "I0222 12:33:38.481530 133585517813760 efficientnet_model.py:144] round_filter input=24 output=48\n",
            "I0222 12:33:39.163190 133585517813760 efficientnet_model.py:144] round_filter input=24 output=48\n",
            "I0222 12:33:39.163403 133585517813760 efficientnet_model.py:144] round_filter input=40 output=80\n",
            "I0222 12:33:39.925399 133585517813760 efficientnet_model.py:144] round_filter input=40 output=80\n",
            "I0222 12:33:39.925612 133585517813760 efficientnet_model.py:144] round_filter input=80 output=160\n",
            "I0222 12:33:40.733835 133585517813760 efficientnet_model.py:144] round_filter input=80 output=160\n",
            "I0222 12:33:40.733998 133585517813760 efficientnet_model.py:144] round_filter input=112 output=224\n",
            "I0222 12:33:41.830588 133585517813760 efficientnet_model.py:144] round_filter input=112 output=224\n",
            "I0222 12:33:41.830810 133585517813760 efficientnet_model.py:144] round_filter input=192 output=384\n",
            "I0222 12:33:42.832816 133585517813760 efficientnet_model.py:144] round_filter input=192 output=384\n",
            "I0222 12:33:42.832979 133585517813760 efficientnet_model.py:144] round_filter input=320 output=640\n",
            "I0222 12:33:43.131551 133585517813760 efficientnet_model.py:144] round_filter input=1280 output=2560\n",
            "I0222 12:33:43.170626 133585517813760 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 22.3s\n",
            "I0222 12:33:43.287564 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 22.3s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I0222 12:33:43.300090 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0222 12:33:43.301780 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0222 12:33:43.302269 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0222 12:33:43.303662 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0222 12:33:43.304922 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0222 12:33:43.305387 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0222 12:33:43.306339 133585517813760 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 28.599s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<EOF >> /content/labelmap.txt\n",
        "blob_normal\n",
        "blob_atento\n",
        "blob_surpreso\n",
        "blob_dormindo\n",
        "blob_feliz\n",
        "blob_triste\n",
        "blob_raiva\n",
        "blob_entediado\n",
        "EOF"
      ],
      "metadata": {
        "id": "ftjwiLDBWst1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data conversion scripts\n",
        "! wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_csv.py\n",
        "! wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_tfrecord.py"
      ],
      "metadata": {
        "id": "laZZE0TlEeUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4a9a54-94f6-42c6-99cd-b7183aac7531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-22 12:33:55--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_csv.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1348 (1.3K) [text/plain]\n",
            "Saving to: ‘create_csv.py’\n",
            "\n",
            "\rcreate_csv.py         0%[                    ]       0  --.-KB/s               \rcreate_csv.py       100%[===================>]   1.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-22 12:33:55 (101 MB/s) - ‘create_csv.py’ saved [1348/1348]\n",
            "\n",
            "--2024-02-22 12:33:55--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_tfrecord.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4414 (4.3K) [text/plain]\n",
            "Saving to: ‘create_tfrecord.py’\n",
            "\n",
            "create_tfrecord.py  100%[===================>]   4.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-22 12:33:55 (65.1 MB/s) - ‘create_tfrecord.py’ saved [4414/4414]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tdDbTmHYwu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ba920b-6af4-4db2-965a-64c09aa49d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n"
          ]
        }
      ],
      "source": [
        "# Script to create CSV data file from Pascal VOC annotation files\n",
        "# Based off code from GitHub user datitran: https://github.com/datitran/raccoon_dataset/blob/master/xml_to_csv.py\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text)\n",
        "                     )\n",
        "            xml_list.append(value)\n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df\n",
        "\n",
        "def main():\n",
        "    for folder in ['train','validation']:\n",
        "        xml_df = xml_to_csv(folder)\n",
        "        xml_df.to_csv((folder + '_labels.csv'), index=None)\n",
        "        print('Successfully converted xml to csv.')\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsrweajZZBDm",
        "outputId": "9df24219-a2ff-417d-831a-58c0efc82ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecords: /content/train.tfrecord\n",
            "Successfully created the TFRecords: /content/val.tfrecord\n"
          ]
        }
      ],
      "source": [
        "# Create CSV data files and TFRecord files\n",
        "!python3 create_tfrecord.py --csv_input=train_labels.csv --labelmap=labelmap.txt --image_dir=train --output_path=train.tfrecord\n",
        "!python3 create_tfrecord.py --csv_input=validation_labels.csv --labelmap=labelmap.txt --image_dir=validation --output_path=val.tfrecord"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_record_fname = '/content/train.tfrecord'\n",
        "val_record_fname = '/content/val.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/labelmap.pbtxt'"
      ],
      "metadata": {
        "id": "1M6GV8VIZk3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the chosen_model variable to deploy different models available in the TF2 object detection zoo\n",
        "chosen_model = 'ssd-mobilenet-v2-fpnlite-320'\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd-mobilenet-v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "    },\n",
        "    'ssd-mobilenet-v2-fpnlite-320': {\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    # The centernet model isn't working as of 9/10/22\n",
        "    #'centernet-mobilenet-v2': {\n",
        "    #    'model_name': 'centernet_mobilenetv2fpn_512x512_coco17_od',\n",
        "    #    'base_pipeline_file': 'pipeline.config',\n",
        "    #    'pretrained_checkpoint': 'centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz',\n",
        "    #}\n",
        "}\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
      ],
      "metadata": {
        "id": "0fnDHxJEa3ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n",
        "%mkdir /content/models/mymodel/\n",
        "%cd /content/models/mymodel/\n",
        "\n",
        "# Download pre-trained model weights\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# Download training configuration file for model\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget {download_config}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5clMTc21a_Rr",
        "outputId": "f2dcaa00-d734-419b-dccd-cc93f6a3b54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/mymodel\n",
            "--2024-02-22 12:34:30--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.124.207, 142.250.152.207, 142.250.128.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.124.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20515344 (20M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’\n",
            "\n",
            "ssd_mobilenet_v2_fp 100%[===================>]  19.56M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-02-22 12:34:30 (134 MB/s) - ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’ saved [20515344/20515344]\n",
            "\n",
            "--2024-02-22 12:34:30--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4684 (4.6K) [text/plain]\n",
            "Saving to: ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config’\n",
            "\n",
            "ssd_mobilenet_v2_fp 100%[===================>]   4.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-22 12:34:30 (72.4 MB/s) - ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config’ saved [4684/4684]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters for the model\n",
        "num_steps = 5000\n",
        "\n",
        "if chosen_model == 'efficientdet-d0':\n",
        "  batch_size = 4\n",
        "else:\n",
        "  batch_size = 16"
      ],
      "metadata": {
        "id": "B_ZmtAILbRix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set file locations and get number of classes for config file\n",
        "pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "print('Total classes:', num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jOJ63vubZ7u",
        "outputId": "c5bb2e6f-507d-41e7-bcf8-719bd3985ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total classes: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom configuration file by writing the dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "import re\n",
        "\n",
        "%cd /content/models/mymodel\n",
        "print('writing custom configuration file')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open('pipeline_file.config', 'w') as f:\n",
        "\n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "\n",
        "    # Set tfrecord files for train and test datasets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
        "\n",
        "    # Set label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set batch_size\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "\n",
        "    # Set number of classes num_classes\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "\n",
        "    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "\n",
        "    # If using ssd-mobilenet-v2, reduce learning rate (because it's too high in the default config file)\n",
        "    if chosen_model == 'ssd-mobilenet-v2':\n",
        "      s = re.sub('learning_rate_base: .8',\n",
        "                 'learning_rate_base: .08', s)\n",
        "\n",
        "      s = re.sub('warmup_learning_rate: 0.13333',\n",
        "                 'warmup_learning_rate: .026666', s)\n",
        "\n",
        "    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n",
        "    if chosen_model == 'efficientdet-d0':\n",
        "      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
        "      s = re.sub('pad_to_max_dimension: true', '', s)\n",
        "      s = re.sub('min_dimension', 'height', s)\n",
        "      s = re.sub('max_dimension', 'width', s)\n",
        "\n",
        "    f.write(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hONATGwkbkZP",
        "outputId": "6a2d25db-ce66-48bd-e8ff-e9f55f3503c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/mymodel\n",
            "writing custom configuration file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Display the custom configuration file's contents\n",
        "!cat /content/models/mymodel/pipeline_file.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRWLVx3kbzp3",
        "outputId": "83939f16-2e6a-4abf-8cce-db8f6165574e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# SSD with Mobilenet v2 FPN-lite (go/fpn-lite) feature extractor, shared box\n",
            "# predictor and focal loss (a mobile version of Retinanet).\n",
            "# Retinanet: see Lin et al, https://arxiv.org/abs/1708.02002\n",
            "# Trained on COCO, initialized from Imagenet classification checkpoint\n",
            "# Train on TPU-8\n",
            "#\n",
            "# Achieves 22.2 mAP on COCO17 Val\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    num_classes: 8\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: [1.0, 2.0, 0.5]\n",
            "        scales_per_octave: 2\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 320\n",
            "        width: 320\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        depth: 128\n",
            "        class_prediction_bias_init: -4.6\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              stddev: 0.01\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            scale: true,\n",
            "            decay: 0.997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "        num_layers_before_predictor: 4\n",
            "        share_prediction_tower: true\n",
            "        use_depthwise: true\n",
            "        kernel_size: 3\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2_fpn_keras'\n",
            "      use_depthwise: true\n",
            "      fpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        additional_layer_depth: 128\n",
            "      }\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          random_normal_initializer {\n",
            "            stddev: 0.01\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          scale: true,\n",
            "          decay: 0.997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "      override_base_feature_extractor_hyperparams: true\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          alpha: 0.25\n",
            "          gamma: 2.0\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  fine_tune_checkpoint_version: V2\n",
            "  fine_tune_checkpoint: \"/content/models/mymodel/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  batch_size: 16\n",
            "  sync_replicas: true\n",
            "  startup_delay_steps: 0\n",
            "  replicas_to_aggregate: 8\n",
            "  num_steps: 5000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_crop_image {\n",
            "      min_object_covered: 0.0\n",
            "      min_aspect_ratio: 0.75\n",
            "      max_aspect_ratio: 3.0\n",
            "      min_area: 0.75\n",
            "      max_area: 1.0\n",
            "      overlap_thresh: 0.0\n",
            "    }\n",
            "  }\n",
            "  optimizer {\n",
            "    momentum_optimizer: {\n",
            "      learning_rate: {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: .08\n",
            "          total_steps: 50000\n",
            "          warmup_learning_rate: .026666\n",
            "          warmup_steps: 1000\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  label_map_path: \"/content/labelmap.pbtxt\"\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/train.tfrecord\"\n",
            "  }\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  label_map_path: \"/content/labelmap.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_epochs: 1\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/val.tfrecord\"\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the custom config file and the directory to store training checkpoints in\n",
        "pipeline_file = '/content/models/mymodel/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ],
      "metadata": {
        "id": "sEStraQLb8d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training/train'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "57I82Y7gcLQU",
        "outputId": "dc5b574e-e25e-4ebd-8c50-1ba0c022336c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 15259), started 0:50:06 ago. (Use '!kill 15259' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training!\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEux4NsucyZF",
        "outputId": "e7b8e4b1-364c-461f-9579-57492db0f86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n",
            "2024-02-22 12:35:10.908382: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I0222 12:35:10.917507 140697476218880 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 5000\n",
            "I0222 12:35:10.924496 140697476218880 config_util.py:552] Maybe overwriting train_steps: 5000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0222 12:35:10.924661 140697476218880 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0222 12:35:10.967427 140697476218880 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/train.tfrecord']\n",
            "I0222 12:35:10.981540 140697476218880 dataset_builder.py:162] Reading unweighted datasets: ['/content/train.tfrecord']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/train.tfrecord']\n",
            "I0222 12:35:10.981755 140697476218880 dataset_builder.py:79] Reading record datasets for input file: ['/content/train.tfrecord']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0222 12:35:10.981851 140697476218880 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0222 12:35:10.982867 140697476218880 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W0222 12:35:10.990533 140697476218880 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0222 12:35:11.201644 140697476218880 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0222 12:35:18.060710 140697476218880 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0222 12:35:20.728971 140697476218880 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0222 12:35:22.106901 140697476218880 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
            "I0222 12:35:36.648927 140692005295680 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "I0222 12:35:49.392941 140692005295680 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "2024-02-22 12:35:57.566155: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
            "\n",
            "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.975258 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.976591 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.978685 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.979578 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.981710 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.982578 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.984534 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.985372 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.987285 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0222 12:36:01.988055 140697476218880 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W0222 12:36:02.544022 140692022081088 deprecation.py:541] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "I0222 12:36:03.571869 140692022081088 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "I0222 12:36:12.781430 140692022081088 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "I0222 12:36:21.051454 140692022081088 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "I0222 12:36:30.555606 140692022081088 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "INFO:tensorflow:Step 100 per-step time 1.077s\n",
            "I0222 12:37:49.928946 140697476218880 model_lib_v2.py:705] Step 100 per-step time 1.077s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.3273121,\n",
            " 'Loss/localization_loss': 0.06509896,\n",
            " 'Loss/regularization_loss': 0.15323028,\n",
            " 'Loss/total_loss': 0.54564136,\n",
            " 'learning_rate': 0.0319994}\n",
            "I0222 12:37:49.929419 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.3273121,\n",
            " 'Loss/localization_loss': 0.06509896,\n",
            " 'Loss/regularization_loss': 0.15323028,\n",
            " 'Loss/total_loss': 0.54564136,\n",
            " 'learning_rate': 0.0319994}\n",
            "INFO:tensorflow:Step 300 per-step time 0.583s\n",
            "I0222 12:39:43.739468 140697476218880 model_lib_v2.py:705] Step 300 per-step time 0.583s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.17036156,\n",
            " 'Loss/localization_loss': 0.047338735,\n",
            " 'Loss/regularization_loss': 0.15243101,\n",
            " 'Loss/total_loss': 0.3701313,\n",
            " 'learning_rate': 0.0426662}\n",
            "I0222 12:39:43.739861 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.17036156,\n",
            " 'Loss/localization_loss': 0.047338735,\n",
            " 'Loss/regularization_loss': 0.15243101,\n",
            " 'Loss/total_loss': 0.3701313,\n",
            " 'learning_rate': 0.0426662}\n",
            "INFO:tensorflow:Step 400 per-step time 0.552s\n",
            "I0222 12:40:38.930265 140697476218880 model_lib_v2.py:705] Step 400 per-step time 0.552s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.14920945,\n",
            " 'Loss/localization_loss': 0.05131723,\n",
            " 'Loss/regularization_loss': 0.15196843,\n",
            " 'Loss/total_loss': 0.35249513,\n",
            " 'learning_rate': 0.047999598}\n",
            "I0222 12:40:38.930645 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.14920945,\n",
            " 'Loss/localization_loss': 0.05131723,\n",
            " 'Loss/regularization_loss': 0.15196843,\n",
            " 'Loss/total_loss': 0.35249513,\n",
            " 'learning_rate': 0.047999598}\n",
            "INFO:tensorflow:Step 500 per-step time 0.575s\n",
            "I0222 12:41:36.478858 140697476218880 model_lib_v2.py:705] Step 500 per-step time 0.575s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.103241295,\n",
            " 'Loss/localization_loss': 0.035559982,\n",
            " 'Loss/regularization_loss': 0.15146701,\n",
            " 'Loss/total_loss': 0.2902683,\n",
            " 'learning_rate': 0.053333}\n",
            "I0222 12:41:36.479281 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.103241295,\n",
            " 'Loss/localization_loss': 0.035559982,\n",
            " 'Loss/regularization_loss': 0.15146701,\n",
            " 'Loss/total_loss': 0.2902683,\n",
            " 'learning_rate': 0.053333}\n",
            "INFO:tensorflow:Step 600 per-step time 0.547s\n",
            "I0222 12:42:31.138795 140697476218880 model_lib_v2.py:705] Step 600 per-step time 0.547s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.111743666,\n",
            " 'Loss/localization_loss': 0.031951424,\n",
            " 'Loss/regularization_loss': 0.15092301,\n",
            " 'Loss/total_loss': 0.2946181,\n",
            " 'learning_rate': 0.0586664}\n",
            "I0222 12:42:31.139179 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.111743666,\n",
            " 'Loss/localization_loss': 0.031951424,\n",
            " 'Loss/regularization_loss': 0.15092301,\n",
            " 'Loss/total_loss': 0.2946181,\n",
            " 'learning_rate': 0.0586664}\n",
            "INFO:tensorflow:Step 700 per-step time 0.567s\n",
            "I0222 12:43:27.899714 140697476218880 model_lib_v2.py:705] Step 700 per-step time 0.567s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.08903312,\n",
            " 'Loss/localization_loss': 0.033231515,\n",
            " 'Loss/regularization_loss': 0.15032107,\n",
            " 'Loss/total_loss': 0.2725857,\n",
            " 'learning_rate': 0.0639998}\n",
            "I0222 12:43:27.900099 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.08903312,\n",
            " 'Loss/localization_loss': 0.033231515,\n",
            " 'Loss/regularization_loss': 0.15032107,\n",
            " 'Loss/total_loss': 0.2725857,\n",
            " 'learning_rate': 0.0639998}\n",
            "INFO:tensorflow:Step 800 per-step time 0.553s\n",
            "I0222 12:44:23.178723 140697476218880 model_lib_v2.py:705] Step 800 per-step time 0.553s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.078081064,\n",
            " 'Loss/localization_loss': 0.03453462,\n",
            " 'Loss/regularization_loss': 0.14966656,\n",
            " 'Loss/total_loss': 0.26228225,\n",
            " 'learning_rate': 0.069333196}\n",
            "I0222 12:44:23.179180 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.078081064,\n",
            " 'Loss/localization_loss': 0.03453462,\n",
            " 'Loss/regularization_loss': 0.14966656,\n",
            " 'Loss/total_loss': 0.26228225,\n",
            " 'learning_rate': 0.069333196}\n",
            "INFO:tensorflow:Step 900 per-step time 0.563s\n",
            "I0222 12:45:19.461788 140697476218880 model_lib_v2.py:705] Step 900 per-step time 0.563s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.07679436,\n",
            " 'Loss/localization_loss': 0.023324585,\n",
            " 'Loss/regularization_loss': 0.14897235,\n",
            " 'Loss/total_loss': 0.2490913,\n",
            " 'learning_rate': 0.074666604}\n",
            "I0222 12:45:19.462207 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.07679436,\n",
            " 'Loss/localization_loss': 0.023324585,\n",
            " 'Loss/regularization_loss': 0.14897235,\n",
            " 'Loss/total_loss': 0.2490913,\n",
            " 'learning_rate': 0.074666604}\n",
            "INFO:tensorflow:Step 1000 per-step time 0.563s\n",
            "I0222 12:46:15.742565 140697476218880 model_lib_v2.py:705] Step 1000 per-step time 0.563s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.067493744,\n",
            " 'Loss/localization_loss': 0.022146191,\n",
            " 'Loss/regularization_loss': 0.14818388,\n",
            " 'Loss/total_loss': 0.23782381,\n",
            " 'learning_rate': 0.08}\n",
            "I0222 12:46:15.742933 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.067493744,\n",
            " 'Loss/localization_loss': 0.022146191,\n",
            " 'Loss/regularization_loss': 0.14818388,\n",
            " 'Loss/total_loss': 0.23782381,\n",
            " 'learning_rate': 0.08}\n",
            "INFO:tensorflow:Step 1100 per-step time 0.562s\n",
            "I0222 12:47:11.910309 140697476218880 model_lib_v2.py:705] Step 1100 per-step time 0.562s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.07419958,\n",
            " 'Loss/localization_loss': 0.031582035,\n",
            " 'Loss/regularization_loss': 0.14739881,\n",
            " 'Loss/total_loss': 0.25318044,\n",
            " 'learning_rate': 0.07999918}\n",
            "I0222 12:47:11.910672 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.07419958,\n",
            " 'Loss/localization_loss': 0.031582035,\n",
            " 'Loss/regularization_loss': 0.14739881,\n",
            " 'Loss/total_loss': 0.25318044,\n",
            " 'learning_rate': 0.07999918}\n",
            "INFO:tensorflow:Step 1200 per-step time 0.595s\n",
            "I0222 12:48:11.435996 140697476218880 model_lib_v2.py:705] Step 1200 per-step time 0.595s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.0557323,\n",
            " 'Loss/localization_loss': 0.019943167,\n",
            " 'Loss/regularization_loss': 0.14656512,\n",
            " 'Loss/total_loss': 0.2222406,\n",
            " 'learning_rate': 0.079996705}\n",
            "I0222 12:48:11.436375 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.0557323,\n",
            " 'Loss/localization_loss': 0.019943167,\n",
            " 'Loss/regularization_loss': 0.14656512,\n",
            " 'Loss/total_loss': 0.2222406,\n",
            " 'learning_rate': 0.079996705}\n",
            "INFO:tensorflow:Step 1300 per-step time 0.565s\n",
            "I0222 12:49:07.949558 140697476218880 model_lib_v2.py:705] Step 1300 per-step time 0.565s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.051616136,\n",
            " 'Loss/localization_loss': 0.015800597,\n",
            " 'Loss/regularization_loss': 0.14573988,\n",
            " 'Loss/total_loss': 0.21315661,\n",
            " 'learning_rate': 0.0799926}\n",
            "I0222 12:49:07.949998 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.051616136,\n",
            " 'Loss/localization_loss': 0.015800597,\n",
            " 'Loss/regularization_loss': 0.14573988,\n",
            " 'Loss/total_loss': 0.21315661,\n",
            " 'learning_rate': 0.0799926}\n",
            "INFO:tensorflow:Step 1400 per-step time 0.561s\n",
            "I0222 12:50:04.086709 140697476218880 model_lib_v2.py:705] Step 1400 per-step time 0.561s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.058298457,\n",
            " 'Loss/localization_loss': 0.021144822,\n",
            " 'Loss/regularization_loss': 0.1449674,\n",
            " 'Loss/total_loss': 0.22441068,\n",
            " 'learning_rate': 0.07998685}\n",
            "I0222 12:50:04.087193 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.058298457,\n",
            " 'Loss/localization_loss': 0.021144822,\n",
            " 'Loss/regularization_loss': 0.1449674,\n",
            " 'Loss/total_loss': 0.22441068,\n",
            " 'learning_rate': 0.07998685}\n",
            "INFO:tensorflow:Step 1500 per-step time 0.569s\n",
            "I0222 12:51:00.985755 140697476218880 model_lib_v2.py:705] Step 1500 per-step time 0.569s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.051161103,\n",
            " 'Loss/localization_loss': 0.016232152,\n",
            " 'Loss/regularization_loss': 0.14416419,\n",
            " 'Loss/total_loss': 0.21155745,\n",
            " 'learning_rate': 0.07997945}\n",
            "I0222 12:51:00.986341 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.051161103,\n",
            " 'Loss/localization_loss': 0.016232152,\n",
            " 'Loss/regularization_loss': 0.14416419,\n",
            " 'Loss/total_loss': 0.21155745,\n",
            " 'learning_rate': 0.07997945}\n",
            "INFO:tensorflow:Step 1600 per-step time 0.542s\n",
            "I0222 12:51:55.134808 140697476218880 model_lib_v2.py:705] Step 1600 per-step time 0.542s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.049683318,\n",
            " 'Loss/localization_loss': 0.012248942,\n",
            " 'Loss/regularization_loss': 0.14332117,\n",
            " 'Loss/total_loss': 0.20525342,\n",
            " 'learning_rate': 0.079970405}\n",
            "I0222 12:51:55.135273 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.049683318,\n",
            " 'Loss/localization_loss': 0.012248942,\n",
            " 'Loss/regularization_loss': 0.14332117,\n",
            " 'Loss/total_loss': 0.20525342,\n",
            " 'learning_rate': 0.079970405}\n",
            "INFO:tensorflow:Step 1700 per-step time 0.578s\n",
            "I0222 12:52:52.981603 140697476218880 model_lib_v2.py:705] Step 1700 per-step time 0.578s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.04248844,\n",
            " 'Loss/localization_loss': 0.012083564,\n",
            " 'Loss/regularization_loss': 0.1424767,\n",
            " 'Loss/total_loss': 0.1970487,\n",
            " 'learning_rate': 0.07995972}\n",
            "I0222 12:52:52.982056 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.04248844,\n",
            " 'Loss/localization_loss': 0.012083564,\n",
            " 'Loss/regularization_loss': 0.1424767,\n",
            " 'Loss/total_loss': 0.1970487,\n",
            " 'learning_rate': 0.07995972}\n",
            "INFO:tensorflow:Step 1800 per-step time 0.532s\n",
            "I0222 12:53:46.227021 140697476218880 model_lib_v2.py:705] Step 1800 per-step time 0.532s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.052521918,\n",
            " 'Loss/localization_loss': 0.016043905,\n",
            " 'Loss/regularization_loss': 0.14185704,\n",
            " 'Loss/total_loss': 0.21042287,\n",
            " 'learning_rate': 0.0799474}\n",
            "I0222 12:53:46.227407 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.052521918,\n",
            " 'Loss/localization_loss': 0.016043905,\n",
            " 'Loss/regularization_loss': 0.14185704,\n",
            " 'Loss/total_loss': 0.21042287,\n",
            " 'learning_rate': 0.0799474}\n",
            "INFO:tensorflow:Step 1900 per-step time 0.552s\n",
            "I0222 12:54:41.387418 140697476218880 model_lib_v2.py:705] Step 1900 per-step time 0.552s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.038488004,\n",
            " 'Loss/localization_loss': 0.01247094,\n",
            " 'Loss/regularization_loss': 0.1410279,\n",
            " 'Loss/total_loss': 0.19198684,\n",
            " 'learning_rate': 0.07993342}\n",
            "I0222 12:54:41.390551 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.038488004,\n",
            " 'Loss/localization_loss': 0.01247094,\n",
            " 'Loss/regularization_loss': 0.1410279,\n",
            " 'Loss/total_loss': 0.19198684,\n",
            " 'learning_rate': 0.07993342}\n",
            "INFO:tensorflow:Step 2000 per-step time 0.545s\n",
            "I0222 12:55:35.845767 140697476218880 model_lib_v2.py:705] Step 2000 per-step time 0.545s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.043066975,\n",
            " 'Loss/localization_loss': 0.011628033,\n",
            " 'Loss/regularization_loss': 0.14022294,\n",
            " 'Loss/total_loss': 0.19491795,\n",
            " 'learning_rate': 0.07991781}\n",
            "I0222 12:55:35.846276 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.043066975,\n",
            " 'Loss/localization_loss': 0.011628033,\n",
            " 'Loss/regularization_loss': 0.14022294,\n",
            " 'Loss/total_loss': 0.19491795,\n",
            " 'learning_rate': 0.07991781}\n",
            "INFO:tensorflow:Step 2100 per-step time 0.539s\n",
            "I0222 12:56:29.709318 140697476218880 model_lib_v2.py:705] Step 2100 per-step time 0.539s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.041210257,\n",
            " 'Loss/localization_loss': 0.012187038,\n",
            " 'Loss/regularization_loss': 0.13942179,\n",
            " 'Loss/total_loss': 0.19281909,\n",
            " 'learning_rate': 0.07990056}\n",
            "I0222 12:56:29.709775 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.041210257,\n",
            " 'Loss/localization_loss': 0.012187038,\n",
            " 'Loss/regularization_loss': 0.13942179,\n",
            " 'Loss/total_loss': 0.19281909,\n",
            " 'learning_rate': 0.07990056}\n",
            "INFO:tensorflow:Step 2200 per-step time 0.579s\n",
            "I0222 12:57:27.613625 140697476218880 model_lib_v2.py:705] Step 2200 per-step time 0.579s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.059629314,\n",
            " 'Loss/localization_loss': 0.0096129915,\n",
            " 'Loss/regularization_loss': 0.13869068,\n",
            " 'Loss/total_loss': 0.20793298,\n",
            " 'learning_rate': 0.07988167}\n",
            "I0222 12:57:27.614006 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.059629314,\n",
            " 'Loss/localization_loss': 0.0096129915,\n",
            " 'Loss/regularization_loss': 0.13869068,\n",
            " 'Loss/total_loss': 0.20793298,\n",
            " 'learning_rate': 0.07988167}\n",
            "INFO:tensorflow:Step 2300 per-step time 0.542s\n",
            "I0222 12:58:21.804054 140697476218880 model_lib_v2.py:705] Step 2300 per-step time 0.542s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.0383256,\n",
            " 'Loss/localization_loss': 0.011519986,\n",
            " 'Loss/regularization_loss': 0.1381847,\n",
            " 'Loss/total_loss': 0.18803029,\n",
            " 'learning_rate': 0.07986114}\n",
            "I0222 12:58:21.804435 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.0383256,\n",
            " 'Loss/localization_loss': 0.011519986,\n",
            " 'Loss/regularization_loss': 0.1381847,\n",
            " 'Loss/total_loss': 0.18803029,\n",
            " 'learning_rate': 0.07986114}\n",
            "INFO:tensorflow:Step 2400 per-step time 0.569s\n",
            "I0222 12:59:18.697871 140697476218880 model_lib_v2.py:705] Step 2400 per-step time 0.569s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.037357956,\n",
            " 'Loss/localization_loss': 0.009071991,\n",
            " 'Loss/regularization_loss': 0.13739234,\n",
            " 'Loss/total_loss': 0.18382229,\n",
            " 'learning_rate': 0.07983897}\n",
            "I0222 12:59:18.698249 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.037357956,\n",
            " 'Loss/localization_loss': 0.009071991,\n",
            " 'Loss/regularization_loss': 0.13739234,\n",
            " 'Loss/total_loss': 0.18382229,\n",
            " 'learning_rate': 0.07983897}\n",
            "INFO:tensorflow:Step 2500 per-step time 0.544s\n",
            "I0222 13:00:13.135180 140697476218880 model_lib_v2.py:705] Step 2500 per-step time 0.544s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.038188506,\n",
            " 'Loss/localization_loss': 0.012204347,\n",
            " 'Loss/regularization_loss': 0.13656905,\n",
            " 'Loss/total_loss': 0.1869619,\n",
            " 'learning_rate': 0.079815164}\n",
            "I0222 13:00:13.135603 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.038188506,\n",
            " 'Loss/localization_loss': 0.012204347,\n",
            " 'Loss/regularization_loss': 0.13656905,\n",
            " 'Loss/total_loss': 0.1869619,\n",
            " 'learning_rate': 0.079815164}\n",
            "INFO:tensorflow:Step 2600 per-step time 0.575s\n",
            "I0222 13:01:10.590829 140697476218880 model_lib_v2.py:705] Step 2600 per-step time 0.575s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.033885743,\n",
            " 'Loss/localization_loss': 0.00802209,\n",
            " 'Loss/regularization_loss': 0.13574667,\n",
            " 'Loss/total_loss': 0.1776545,\n",
            " 'learning_rate': 0.07978972}\n",
            "I0222 13:01:10.597474 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.033885743,\n",
            " 'Loss/localization_loss': 0.00802209,\n",
            " 'Loss/regularization_loss': 0.13574667,\n",
            " 'Loss/total_loss': 0.1776545,\n",
            " 'learning_rate': 0.07978972}\n",
            "INFO:tensorflow:Step 2700 per-step time 0.550s\n",
            "I0222 13:02:05.583148 140697476218880 model_lib_v2.py:705] Step 2700 per-step time 0.550s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.036444493,\n",
            " 'Loss/localization_loss': 0.008085304,\n",
            " 'Loss/regularization_loss': 0.1349342,\n",
            " 'Loss/total_loss': 0.179464,\n",
            " 'learning_rate': 0.07976264}\n",
            "I0222 13:02:05.583582 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.036444493,\n",
            " 'Loss/localization_loss': 0.008085304,\n",
            " 'Loss/regularization_loss': 0.1349342,\n",
            " 'Loss/total_loss': 0.179464,\n",
            " 'learning_rate': 0.07976264}\n",
            "INFO:tensorflow:Step 2800 per-step time 0.548s\n",
            "I0222 13:03:00.331472 140697476218880 model_lib_v2.py:705] Step 2800 per-step time 0.548s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.03385789,\n",
            " 'Loss/localization_loss': 0.007931871,\n",
            " 'Loss/regularization_loss': 0.13412866,\n",
            " 'Loss/total_loss': 0.17591843,\n",
            " 'learning_rate': 0.07973392}\n",
            "I0222 13:03:00.331844 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.03385789,\n",
            " 'Loss/localization_loss': 0.007931871,\n",
            " 'Loss/regularization_loss': 0.13412866,\n",
            " 'Loss/total_loss': 0.17591843,\n",
            " 'learning_rate': 0.07973392}\n",
            "INFO:tensorflow:Step 2900 per-step time 0.545s\n",
            "I0222 13:03:54.809436 140697476218880 model_lib_v2.py:705] Step 2900 per-step time 0.545s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.035306323,\n",
            " 'Loss/localization_loss': 0.010864093,\n",
            " 'Loss/regularization_loss': 0.1334008,\n",
            " 'Loss/total_loss': 0.17957121,\n",
            " 'learning_rate': 0.07970358}\n",
            "I0222 13:03:54.809862 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.035306323,\n",
            " 'Loss/localization_loss': 0.010864093,\n",
            " 'Loss/regularization_loss': 0.1334008,\n",
            " 'Loss/total_loss': 0.17957121,\n",
            " 'learning_rate': 0.07970358}\n",
            "INFO:tensorflow:Step 3000 per-step time 0.551s\n",
            "I0222 13:04:49.884357 140697476218880 model_lib_v2.py:705] Step 3000 per-step time 0.551s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.029882902,\n",
            " 'Loss/localization_loss': 0.0056894515,\n",
            " 'Loss/regularization_loss': 0.13261256,\n",
            " 'Loss/total_loss': 0.1681849,\n",
            " 'learning_rate': 0.0796716}\n",
            "I0222 13:04:49.885554 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.029882902,\n",
            " 'Loss/localization_loss': 0.0056894515,\n",
            " 'Loss/regularization_loss': 0.13261256,\n",
            " 'Loss/total_loss': 0.1681849,\n",
            " 'learning_rate': 0.0796716}\n",
            "INFO:tensorflow:Step 3100 per-step time 0.565s\n",
            "I0222 13:05:46.408003 140697476218880 model_lib_v2.py:705] Step 3100 per-step time 0.565s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.03305194,\n",
            " 'Loss/localization_loss': 0.0073116566,\n",
            " 'Loss/regularization_loss': 0.13182156,\n",
            " 'Loss/total_loss': 0.17218515,\n",
            " 'learning_rate': 0.07963799}\n",
            "I0222 13:05:46.408396 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.03305194,\n",
            " 'Loss/localization_loss': 0.0073116566,\n",
            " 'Loss/regularization_loss': 0.13182156,\n",
            " 'Loss/total_loss': 0.17218515,\n",
            " 'learning_rate': 0.07963799}\n",
            "INFO:tensorflow:Step 3200 per-step time 0.563s\n",
            "I0222 13:06:42.670201 140697476218880 model_lib_v2.py:705] Step 3200 per-step time 0.563s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.028179849,\n",
            " 'Loss/localization_loss': 0.0056729,\n",
            " 'Loss/regularization_loss': 0.13103463,\n",
            " 'Loss/total_loss': 0.16488737,\n",
            " 'learning_rate': 0.07960275}\n",
            "I0222 13:06:42.670601 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.028179849,\n",
            " 'Loss/localization_loss': 0.0056729,\n",
            " 'Loss/regularization_loss': 0.13103463,\n",
            " 'Loss/total_loss': 0.16488737,\n",
            " 'learning_rate': 0.07960275}\n",
            "INFO:tensorflow:Step 3300 per-step time 0.542s\n",
            "I0222 13:07:36.833122 140697476218880 model_lib_v2.py:705] Step 3300 per-step time 0.542s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.034283847,\n",
            " 'Loss/localization_loss': 0.008035843,\n",
            " 'Loss/regularization_loss': 0.13024911,\n",
            " 'Loss/total_loss': 0.1725688,\n",
            " 'learning_rate': 0.07956588}\n",
            "I0222 13:07:36.833621 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.034283847,\n",
            " 'Loss/localization_loss': 0.008035843,\n",
            " 'Loss/regularization_loss': 0.13024911,\n",
            " 'Loss/total_loss': 0.1725688,\n",
            " 'learning_rate': 0.07956588}\n",
            "INFO:tensorflow:Step 3400 per-step time 0.525s\n",
            "I0222 13:08:29.344897 140697476218880 model_lib_v2.py:705] Step 3400 per-step time 0.525s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.03289135,\n",
            " 'Loss/localization_loss': 0.0058088475,\n",
            " 'Loss/regularization_loss': 0.12949198,\n",
            " 'Loss/total_loss': 0.16819218,\n",
            " 'learning_rate': 0.079527386}\n",
            "I0222 13:08:29.348839 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.03289135,\n",
            " 'Loss/localization_loss': 0.0058088475,\n",
            " 'Loss/regularization_loss': 0.12949198,\n",
            " 'Loss/total_loss': 0.16819218,\n",
            " 'learning_rate': 0.079527386}\n",
            "INFO:tensorflow:Step 3500 per-step time 0.553s\n",
            "I0222 13:09:24.682121 140697476218880 model_lib_v2.py:705] Step 3500 per-step time 0.553s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.03428865,\n",
            " 'Loss/localization_loss': 0.0077838805,\n",
            " 'Loss/regularization_loss': 0.12872134,\n",
            " 'Loss/total_loss': 0.17079386,\n",
            " 'learning_rate': 0.07948727}\n",
            "I0222 13:09:24.682545 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.03428865,\n",
            " 'Loss/localization_loss': 0.0077838805,\n",
            " 'Loss/regularization_loss': 0.12872134,\n",
            " 'Loss/total_loss': 0.17079386,\n",
            " 'learning_rate': 0.07948727}\n",
            "INFO:tensorflow:Step 3600 per-step time 0.536s\n",
            "I0222 13:10:18.283407 140697476218880 model_lib_v2.py:705] Step 3600 per-step time 0.536s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.030393776,\n",
            " 'Loss/localization_loss': 0.007272814,\n",
            " 'Loss/regularization_loss': 0.12795451,\n",
            " 'Loss/total_loss': 0.1656211,\n",
            " 'learning_rate': 0.079445526}\n",
            "I0222 13:10:18.283767 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.030393776,\n",
            " 'Loss/localization_loss': 0.007272814,\n",
            " 'Loss/regularization_loss': 0.12795451,\n",
            " 'Loss/total_loss': 0.1656211,\n",
            " 'learning_rate': 0.079445526}\n",
            "INFO:tensorflow:Step 3700 per-step time 0.539s\n",
            "I0222 13:11:12.217250 140697476218880 model_lib_v2.py:705] Step 3700 per-step time 0.539s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.027865537,\n",
            " 'Loss/localization_loss': 0.0046693855,\n",
            " 'Loss/regularization_loss': 0.1271926,\n",
            " 'Loss/total_loss': 0.15972753,\n",
            " 'learning_rate': 0.07940216}\n",
            "I0222 13:11:12.217694 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.027865537,\n",
            " 'Loss/localization_loss': 0.0046693855,\n",
            " 'Loss/regularization_loss': 0.1271926,\n",
            " 'Loss/total_loss': 0.15972753,\n",
            " 'learning_rate': 0.07940216}\n",
            "INFO:tensorflow:Step 3800 per-step time 0.536s\n",
            "I0222 13:12:05.812411 140697476218880 model_lib_v2.py:705] Step 3800 per-step time 0.536s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.027604295,\n",
            " 'Loss/localization_loss': 0.005934066,\n",
            " 'Loss/regularization_loss': 0.1264341,\n",
            " 'Loss/total_loss': 0.15997246,\n",
            " 'learning_rate': 0.079357184}\n",
            "I0222 13:12:05.812810 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.027604295,\n",
            " 'Loss/localization_loss': 0.005934066,\n",
            " 'Loss/regularization_loss': 0.1264341,\n",
            " 'Loss/total_loss': 0.15997246,\n",
            " 'learning_rate': 0.079357184}\n",
            "INFO:tensorflow:Step 3900 per-step time 0.555s\n",
            "I0222 13:13:01.308217 140697476218880 model_lib_v2.py:705] Step 3900 per-step time 0.555s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.029769665,\n",
            " 'Loss/localization_loss': 0.0058456417,\n",
            " 'Loss/regularization_loss': 0.12568444,\n",
            " 'Loss/total_loss': 0.16129975,\n",
            " 'learning_rate': 0.07931058}\n",
            "I0222 13:13:01.308637 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.029769665,\n",
            " 'Loss/localization_loss': 0.0058456417,\n",
            " 'Loss/regularization_loss': 0.12568444,\n",
            " 'Loss/total_loss': 0.16129975,\n",
            " 'learning_rate': 0.07931058}\n",
            "INFO:tensorflow:Step 4000 per-step time 0.556s\n",
            "I0222 13:13:56.888069 140697476218880 model_lib_v2.py:705] Step 4000 per-step time 0.556s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.033464663,\n",
            " 'Loss/localization_loss': 0.0091208825,\n",
            " 'Loss/regularization_loss': 0.12494483,\n",
            " 'Loss/total_loss': 0.16753037,\n",
            " 'learning_rate': 0.07926236}\n",
            "I0222 13:13:56.888589 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.033464663,\n",
            " 'Loss/localization_loss': 0.0091208825,\n",
            " 'Loss/regularization_loss': 0.12494483,\n",
            " 'Loss/total_loss': 0.16753037,\n",
            " 'learning_rate': 0.07926236}\n",
            "INFO:tensorflow:Step 4100 per-step time 0.539s\n",
            "I0222 13:14:50.769378 140697476218880 model_lib_v2.py:705] Step 4100 per-step time 0.539s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.03101692,\n",
            " 'Loss/localization_loss': 0.0052942727,\n",
            " 'Loss/regularization_loss': 0.12420309,\n",
            " 'Loss/total_loss': 0.1605143,\n",
            " 'learning_rate': 0.07921253}\n",
            "I0222 13:14:50.769767 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.03101692,\n",
            " 'Loss/localization_loss': 0.0052942727,\n",
            " 'Loss/regularization_loss': 0.12420309,\n",
            " 'Loss/total_loss': 0.1605143,\n",
            " 'learning_rate': 0.07921253}\n",
            "INFO:tensorflow:Step 4200 per-step time 0.567s\n",
            "I0222 13:15:47.514667 140697476218880 model_lib_v2.py:705] Step 4200 per-step time 0.567s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.02618452,\n",
            " 'Loss/localization_loss': 0.0063037258,\n",
            " 'Loss/regularization_loss': 0.12346135,\n",
            " 'Loss/total_loss': 0.1559496,\n",
            " 'learning_rate': 0.07916109}\n",
            "I0222 13:15:47.515166 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.02618452,\n",
            " 'Loss/localization_loss': 0.0063037258,\n",
            " 'Loss/regularization_loss': 0.12346135,\n",
            " 'Loss/total_loss': 0.1559496,\n",
            " 'learning_rate': 0.07916109}\n",
            "INFO:tensorflow:Step 4300 per-step time 0.571s\n",
            "I0222 13:16:44.626067 140697476218880 model_lib_v2.py:705] Step 4300 per-step time 0.571s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.029419567,\n",
            " 'Loss/localization_loss': 0.0071419114,\n",
            " 'Loss/regularization_loss': 0.122722834,\n",
            " 'Loss/total_loss': 0.15928431,\n",
            " 'learning_rate': 0.07910804}\n",
            "I0222 13:16:44.626441 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.029419567,\n",
            " 'Loss/localization_loss': 0.0071419114,\n",
            " 'Loss/regularization_loss': 0.122722834,\n",
            " 'Loss/total_loss': 0.15928431,\n",
            " 'learning_rate': 0.07910804}\n",
            "INFO:tensorflow:Step 4400 per-step time 0.542s\n",
            "I0222 13:17:38.816189 140697476218880 model_lib_v2.py:705] Step 4400 per-step time 0.542s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.023482483,\n",
            " 'Loss/localization_loss': 0.005769144,\n",
            " 'Loss/regularization_loss': 0.121994406,\n",
            " 'Loss/total_loss': 0.15124604,\n",
            " 'learning_rate': 0.07905338}\n",
            "I0222 13:17:38.816588 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.023482483,\n",
            " 'Loss/localization_loss': 0.005769144,\n",
            " 'Loss/regularization_loss': 0.121994406,\n",
            " 'Loss/total_loss': 0.15124604,\n",
            " 'learning_rate': 0.07905338}\n",
            "INFO:tensorflow:Step 4500 per-step time 0.550s\n",
            "I0222 13:18:33.874081 140697476218880 model_lib_v2.py:705] Step 4500 per-step time 0.550s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.02501577,\n",
            " 'Loss/localization_loss': 0.0042979587,\n",
            " 'Loss/regularization_loss': 0.12126541,\n",
            " 'Loss/total_loss': 0.15057914,\n",
            " 'learning_rate': 0.07899711}\n",
            "I0222 13:18:33.875226 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.02501577,\n",
            " 'Loss/localization_loss': 0.0042979587,\n",
            " 'Loss/regularization_loss': 0.12126541,\n",
            " 'Loss/total_loss': 0.15057914,\n",
            " 'learning_rate': 0.07899711}\n",
            "INFO:tensorflow:Step 4600 per-step time 0.569s\n",
            "I0222 13:19:30.751229 140697476218880 model_lib_v2.py:705] Step 4600 per-step time 0.569s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.025829138,\n",
            " 'Loss/localization_loss': 0.006710794,\n",
            " 'Loss/regularization_loss': 0.120552994,\n",
            " 'Loss/total_loss': 0.15309292,\n",
            " 'learning_rate': 0.078939244}\n",
            "I0222 13:19:30.751732 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.025829138,\n",
            " 'Loss/localization_loss': 0.006710794,\n",
            " 'Loss/regularization_loss': 0.120552994,\n",
            " 'Loss/total_loss': 0.15309292,\n",
            " 'learning_rate': 0.078939244}\n",
            "INFO:tensorflow:Step 4700 per-step time 0.555s\n",
            "I0222 13:20:26.204287 140697476218880 model_lib_v2.py:705] Step 4700 per-step time 0.555s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.02519332,\n",
            " 'Loss/localization_loss': 0.0049004895,\n",
            " 'Loss/regularization_loss': 0.11984293,\n",
            " 'Loss/total_loss': 0.14993674,\n",
            " 'learning_rate': 0.07887978}\n",
            "I0222 13:20:26.204674 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.02519332,\n",
            " 'Loss/localization_loss': 0.0049004895,\n",
            " 'Loss/regularization_loss': 0.11984293,\n",
            " 'Loss/total_loss': 0.14993674,\n",
            " 'learning_rate': 0.07887978}\n",
            "INFO:tensorflow:Step 4800 per-step time 0.558s\n",
            "I0222 13:21:21.983203 140697476218880 model_lib_v2.py:705] Step 4800 per-step time 0.558s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.02649383,\n",
            " 'Loss/localization_loss': 0.006373783,\n",
            " 'Loss/regularization_loss': 0.11913364,\n",
            " 'Loss/total_loss': 0.15200125,\n",
            " 'learning_rate': 0.07881871}\n",
            "I0222 13:21:21.983835 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.02649383,\n",
            " 'Loss/localization_loss': 0.006373783,\n",
            " 'Loss/regularization_loss': 0.11913364,\n",
            " 'Loss/total_loss': 0.15200125,\n",
            " 'learning_rate': 0.07881871}\n",
            "INFO:tensorflow:Step 4900 per-step time 0.551s\n",
            "I0222 13:22:17.053773 140697476218880 model_lib_v2.py:705] Step 4900 per-step time 0.551s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.025444323,\n",
            " 'Loss/localization_loss': 0.0062102964,\n",
            " 'Loss/regularization_loss': 0.11843097,\n",
            " 'Loss/total_loss': 0.1500856,\n",
            " 'learning_rate': 0.07875605}\n",
            "I0222 13:22:17.054177 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.025444323,\n",
            " 'Loss/localization_loss': 0.0062102964,\n",
            " 'Loss/regularization_loss': 0.11843097,\n",
            " 'Loss/total_loss': 0.1500856,\n",
            " 'learning_rate': 0.07875605}\n",
            "INFO:tensorflow:Step 5000 per-step time 0.577s\n",
            "I0222 13:23:14.782779 140697476218880 model_lib_v2.py:705] Step 5000 per-step time 0.577s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.021551268,\n",
            " 'Loss/localization_loss': 0.0051169484,\n",
            " 'Loss/regularization_loss': 0.11773725,\n",
            " 'Loss/total_loss': 0.14440547,\n",
            " 'learning_rate': 0.078691795}\n",
            "I0222 13:23:14.783199 140697476218880 model_lib_v2.py:708] {'Loss/classification_loss': 0.021551268,\n",
            " 'Loss/localization_loss': 0.0051169484,\n",
            " 'Loss/regularization_loss': 0.11773725,\n",
            " 'Loss/total_loss': 0.14440547,\n",
            " 'learning_rate': 0.078691795}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory to store the trained TFLite model\n",
        "!mkdir /content/custom_model_lite\n",
        "output_directory = '/content/custom_model_lite'\n",
        "\n",
        "# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n",
        "last_model_path = '/content/training'\n",
        "\n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILMELDHwGmfQ",
        "outputId": "3982d2e3-dcfd-405e-fbbf-bdf7bdc0280a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-22 13:56:33.404843: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0222 13:56:37.354618 134176165904384 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "I0222 13:56:43.052643 134176165904384 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "2024-02-22 13:56:44.641776: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "I0222 13:56:45.449419 134176165904384 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7a07d6f455d0>, because it is not built.\n",
            "W0222 13:56:46.708476 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7a07d6f455d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7a07d6f46350>, because it is not built.\n",
            "W0222 13:56:46.952181 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7a07d6f46350>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d01345e0>, because it is not built.\n",
            "W0222 13:56:46.952440 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d01345e0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07d0135090>, because it is not built.\n",
            "W0222 13:56:46.952570 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07d0135090>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7a07d00c6fe0>, because it is not built.\n",
            "W0222 13:56:46.952658 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7a07d00c6fe0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d00c5cc0>, because it is not built.\n",
            "W0222 13:56:46.952748 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d00c5cc0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07d00c6c80>, because it is not built.\n",
            "W0222 13:56:46.952827 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07d00c6c80>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7a07d00c5e40>, because it is not built.\n",
            "W0222 13:56:46.952903 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7a07d00c5e40>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07bc23e200>, because it is not built.\n",
            "W0222 13:56:46.952978 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07bc23e200>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc23d6f0>, because it is not built.\n",
            "W0222 13:56:46.953053 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc23d6f0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7a07b7fbafb0>, because it is not built.\n",
            "W0222 13:56:46.953125 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7a07b7fbafb0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7fbbcd0>, because it is not built.\n",
            "W0222 13:56:46.953197 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7fbbcd0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc277a90>, because it is not built.\n",
            "W0222 13:56:46.953286 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc277a90>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d6f46920>, because it is not built.\n",
            "W0222 13:56:46.953365 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d6f46920>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3263e0>, because it is not built.\n",
            "W0222 13:56:46.953439 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3263e0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad327d00>, because it is not built.\n",
            "W0222 13:56:46.953512 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad327d00>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad324490>, because it is not built.\n",
            "W0222 13:56:46.953594 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad324490>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7fb06d0>, because it is not built.\n",
            "W0222 13:56:46.953668 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7fb06d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07b7fb31c0>, because it is not built.\n",
            "W0222 13:56:46.953742 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07b7fb31c0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07bc2ac8b0>, because it is not built.\n",
            "W0222 13:56:46.953814 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07bc2ac8b0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc2af310>, because it is not built.\n",
            "W0222 13:56:46.953888 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc2af310>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d6f46950>, because it is not built.\n",
            "W0222 13:56:46.953960 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d6f46950>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc2206a0>, because it is not built.\n",
            "W0222 13:56:46.954034 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc2206a0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7f30e80>, because it is not built.\n",
            "W0222 13:56:46.954107 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7f30e80>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07b7f30cd0>, because it is not built.\n",
            "W0222 13:56:46.954187 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07b7f30cd0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7f31960>, because it is not built.\n",
            "W0222 13:56:46.954269 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7f31960>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07b7f32b60>, because it is not built.\n",
            "W0222 13:56:46.954344 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07b7f32b60>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7f33940>, because it is not built.\n",
            "W0222 13:56:46.954424 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07b7f33940>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07b7f335b0>, because it is not built.\n",
            "W0222 13:56:46.954498 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07b7f335b0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d6f46980>, because it is not built.\n",
            "W0222 13:56:46.954570 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07d6f46980>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc264dc0>, because it is not built.\n",
            "W0222 13:56:46.954643 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07bc264dc0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c1600>, because it is not built.\n",
            "W0222 13:56:46.954716 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c1600>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c39d0>, because it is not built.\n",
            "W0222 13:56:46.954789 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c39d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c6c80>, because it is not built.\n",
            "W0222 13:56:46.954861 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c6c80>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c5e70>, because it is not built.\n",
            "W0222 13:56:46.954948 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c5e70>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c50f0>, because it is not built.\n",
            "W0222 13:56:46.955019 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c50f0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c74c0>, because it is not built.\n",
            "W0222 13:56:46.955099 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c74c0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c4340>, because it is not built.\n",
            "W0222 13:56:46.955181 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c4340>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c61a0>, because it is not built.\n",
            "W0222 13:56:46.955264 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c61a0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c6d70>, because it is not built.\n",
            "W0222 13:56:46.955339 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c6d70>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c48e0>, because it is not built.\n",
            "W0222 13:56:46.955448 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c48e0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c5030>, because it is not built.\n",
            "W0222 13:56:46.955542 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c5030>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c5cc0>, because it is not built.\n",
            "W0222 13:56:46.955637 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c5cc0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c4550>, because it is not built.\n",
            "W0222 13:56:46.955729 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7a07ad3c4550>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c4910>, because it is not built.\n",
            "W0222 13:56:47.031225 134176165904384 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7a07ad3c4910>, because it is not built.\n",
            "W0222 13:57:04.249738 134176165904384 save.py:260] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 104). These functions will not be directly callable after loading.\n",
            "INFO:tensorflow:Assets written to: /content/custom_model_lite/saved_model/assets\n",
            "I0222 13:57:08.545926 134176165904384 builder_impl.py:779] Assets written to: /content/custom_model_lite/saved_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert exported graph file into TFLite model file\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/content/custom_model_lite/detect.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "jIpM-g9Dww0X",
        "outputId": "bcea6a63-dbd2-4695-92ee-0ed9c6a200cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "SavedModel file does not exist at: /content/custom_model_lite/saved_model/{saved_model.pbtxt|saved_model.pb}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7699d522f8ef>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/custom_model_lite/saved_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mfrom_saved_model\u001b[0;34m(cls, saved_model_dir, signature_keys, tags)\u001b[0m\n\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2087\u001b[0;31m       \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignature_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2089\u001b[0m       \u001b[0msignature_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mexport_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m   saved_model_proto, debug_info = (\n\u001b[0;32m-> 1017\u001b[0;31m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \"\"\"\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   debug_info_path = file_io.join(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot parse file {path_to_pbtxt}: {str(e)}.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     raise IOError(\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;34mf\"SavedModel file does not exist at: {export_dir}{os.path.sep}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;34mf\"{{{constants.SAVED_MODEL_FILENAME_PBTXT}|\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /content/custom_model_lite/saved_model/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4WtI8i5K96w"
      },
      "outputs": [],
      "source": [
        "# Script to run custom TFLite model on test images to detect objects\n",
        "# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import importlib.util\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "### Define function for inferencing with TFLite model and displaying results\n",
        "\n",
        "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
        "\n",
        "  # Grab filenames of all images in test folder\n",
        "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
        "\n",
        "  # Load the label map into memory\n",
        "  with open(lblpath, 'r') as f:\n",
        "      labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # Load the Tensorflow Lite model into memory\n",
        "  interpreter = Interpreter(model_path=modelpath)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get model details\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  height = input_details[0]['shape'][1]\n",
        "  width = input_details[0]['shape'][2]\n",
        "\n",
        "  float_input = (input_details[0]['dtype'] == np.float32)\n",
        "\n",
        "  input_mean = 127.5\n",
        "  input_std = 127.5\n",
        "\n",
        "  # Randomly select test images\n",
        "  images_to_test = random.sample(images, num_test_images)\n",
        "\n",
        "  # Loop over every image and perform detection\n",
        "  for image_path in images_to_test:\n",
        "\n",
        "      # Load image and resize to expected shape [1xHxWx3]\n",
        "      image = cv2.imread(image_path)\n",
        "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      imH, imW, _ = image.shape\n",
        "      image_resized = cv2.resize(image_rgb, (width, height))\n",
        "      input_data = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
        "      if float_input:\n",
        "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
        "\n",
        "      # Perform the actual detection by running the model with the image as input\n",
        "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
        "      interpreter.invoke()\n",
        "\n",
        "      # Retrieve detection results\n",
        "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n",
        "      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n",
        "      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n",
        "\n",
        "      detections = []\n",
        "\n",
        "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
        "      for i in range(len(scores)):\n",
        "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
        "\n",
        "              # Get bounding box coordinates and draw box\n",
        "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
        "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
        "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
        "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
        "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
        "\n",
        "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
        "\n",
        "              # Draw label\n",
        "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
        "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
        "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
        "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
        "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
        "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
        "\n",
        "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
        "\n",
        "\n",
        "      # All the results have been drawn on the image, now display the image\n",
        "      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        plt.figure(figsize=(12,16))\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "\n",
        "      # Save detection results in .txt files (for calculating mAP)\n",
        "      elif txt_only == True:\n",
        "\n",
        "        # Get filenames and paths\n",
        "        image_fn = os.path.basename(image_path)\n",
        "        base_fn, ext = os.path.splitext(image_fn)\n",
        "        txt_result_fn = base_fn +'.txt'\n",
        "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
        "\n",
        "        # Write results to text file\n",
        "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
        "        with open(txt_savepath,'w') as f:\n",
        "            for detection in detections:\n",
        "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up variables for running user's model\n",
        "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect.tflite'   # Path to .tflite model file\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n",
        "min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
        "images_to_test = 10   # Number of images to run detection on\n",
        "\n",
        "# Run inferencing function!\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ],
      "metadata": {
        "id": "6t8CMarqBqP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/Cartucho/mAP /content/mAP\n",
        "cd /content/mAP\n",
        "rm input/detection-results/*\n",
        "rm input/ground-truth/*\n",
        "rm input/images-optional/*\n",
        "wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"
      ],
      "metadata": {
        "id": "JlWarXEZDUqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/images/test/* /content/mAP/input/images-optional # Copy images and xml files\n",
        "!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/  # Move xml files to the appropriate folder"
      ],
      "metadata": {
        "id": "5szFfVxwI3wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/mAP/scripts/extra/convert_gt_xml.py"
      ],
      "metadata": {
        "id": "qdjtOUDnK2AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
        "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect.tflite'   # Path to .tflite model file\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n",
        "PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n",
        "min_conf_threshold=0.1   # Confidence threshold\n",
        "\n",
        "# Use all the images in the test folder\n",
        "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
        "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
        "\n",
        "# Tell function to just save results and not display images\n",
        "txt_only = True\n",
        "\n",
        "# Run inferencing function!\n",
        "print('Starting inference on %d images...' % images_to_test)\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
        "print('Finished inferencing!')"
      ],
      "metadata": {
        "id": "szzHFAhsMNFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mAP\n",
        "!python calculate_map_cartucho.py --labels=/content/labelmap.txt"
      ],
      "metadata": {
        "id": "3DkjpIBARTQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awZMQGVqMpVL"
      },
      "outputs": [],
      "source": [
        "# Move labelmap and pipeline config files into TFLite model folder and zip it up\n",
        "!cp /content/labelmap.txt /content/custom_model_lite\n",
        "!cp /content/labelmap.pbtxt /content/custom_model_lite\n",
        "!cp /content/models/mymodel/pipeline_file.config /content/custom_model_lite\n",
        "\n",
        "%cd /content\n",
        "!zip -r custom_model_lite.zip custom_model_lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVPfAGbNPV56"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/custom_model_lite.zip')"
      ]
    }
  ]
}